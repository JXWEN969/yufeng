{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据分析\n",
    "* _id：案例的唯一标识符。\n",
    "* context：案例内容，由法院判决书中的事实描述部分提取，分为标题和句子列表。\n",
    "* question：针对案件提出的问题，每个案件都有一个问题。\n",
    "* answer：问题的回答，可能是片段、YES/NO或\"unknown\"。\n",
    "* supporting_facts：支持回答的问题的依据，包含标题和句子编号。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import json\n",
    "from os.path import join\n",
    "\n",
    "# 处理输入参数，设置最大查询长度和文档长度，并根据指定名称更新检查点和预测路径\n",
    "def process_arguments(args):\n",
    "    args.checkpoint_path = join(args.checkpoint_path, args.name) # 更新检查点路径为\"检查点基路径/项目名称\"\n",
    "    args.prediction_path = join(args.prediction_path, args.name) # 更新预测文件保存路径为\"预测文件基路径/项目名称\"\n",
    "    args.max_query_len = 50  # 最大查询长度设定为50，用于裁判文书问题的最大长度\n",
    "    args.max_doc_len = 512   # 最大文档长度设定为512，用于裁判文书内容的最大长度\n",
    "\n",
    "# 保存运行时的配置参数到JSON文件\n",
    "def save_settings(args):\n",
    "    os.makedirs(args.checkpoint_path, exist_ok=True)  # 创建检查点目录（如果不存在）\n",
    "    os.makedirs(args.prediction_path, exist_ok=True)  # 创建预测结果目录（如果不存在）\n",
    "    json.dump(args.__dict__, open(join(args.checkpoint_path, \"run_settings.json\"), 'w')) # 将配置保存为JSON格式\n",
    "\n",
    "# 设置项目配置参数\n",
    "def set_config():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    data_path = 'output' # 默认输出文件夹路径\n",
    "\n",
    "    # 定义必需和可选的命令行参数\n",
    "    parser.add_argument(\"--name\", type=str, default='default') # 项目名称，默认为\"default\"\n",
    "    parser.add_argument(\"--prediction_path\", type=str, default=join(data_path, 'submissions')) # 预测结果路径，默认为\"output/submissions\"\n",
    "    parser.add_argument(\"--checkpoint_path\", type=str, default=join(data_path, 'checkpoints')) # 检查点路径，默认为\"output/checkpoints\"\n",
    "    parser.add_argument(\"--data_dir\", type=str, default='data') # 数据目录，默认为\"data\"\n",
    "\n",
    "    parser.add_argument(\"--fp16\", action='store_true') # 是否使用FP16精度训练\n",
    "\n",
    "    parser.add_argument(\"--ckpt_id\", type=int, default=0) # 检查点ID，默认为0\n",
    "    parser.add_argument(\"--bert_model\", type=str, default='bert-base-uncased',\n",
    "                        help='Currently only support bert-base-uncased and bert-large-uncased') # 使用的BERT模型，默认为'bert-base-uncased'\n",
    "\n",
    "    # 学习和日志参数\n",
    "    parser.add_argument(\"--epochs\", type=int, default=4) # 训练轮数，默认为4\n",
    "    parser.add_argument(\"--qat_epochs\", type=int, default=0) # 量化感知训练轮数（QAT），默认为0\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32) # 批大小，默认为32\n",
    "    parser.add_argument(\"--max_bert_size\", type=int, default=8) # BERT层的最大尺寸，默认为8\n",
    "    parser.add_argument(\"--eval_batch_size\", type=int, default=32) # 评估时的批大小，默认为32\n",
    "    parser.add_argument(\"--lr\", type=float, default=2e-4) # 学习率，默认为2e-4\n",
    "    parser.add_argument('--decay', type=float, default=1.0) # 学习率衰减率，默认为1.0\n",
    "    parser.add_argument('--early_stop_epoch', type=int, default=0) # 提前停止训练的轮数，默认为0\n",
    "    parser.add_argument(\"--verbose_step\", default=50, type=int) # 显示训练进度的间隔步数，默认每50步显示一次\n",
    "    parser.add_argument(\"--gradient_accumulation_steps\", default=1, type=int) # 梯度累积步数，默认为1\n",
    "    parser.add_argument(\"--seed\", default=0, type=int) # 随机种子，默认为0\n",
    "\n",
    "    parser.add_argument('--q_update', action='store_true', help='Whether update query') # 是否更新查询\n",
    "    parser.add_argument(\"--prediction_trans\", action='store_true', help='transformer version prediction layer') # 是否使用Transformer版本的预测层\n",
    "    parser.add_argument(\"--trans_drop\", type=float, default=0.5) # Transformer层的dropout率，默认为0.5\n",
    "    parser.add_argument(\"--trans_heads\", type=int, default=3) # Transformer层的头数，默认为3\n",
    "\n",
    "    parser.add_argument(\"--input_dim\", type=int, default=768, help=\"bert-base=768, bert-large=1024\") # 输入维度，默认为768\n",
    "\n",
    "    parser.add_argument(\"--model_gpu\", default='0', type=str, help=\"device to place model.\") # 训练模型的GPU编号，默认为'0'\n",
    "    parser.add_argument('--trained_weight',default=None) # 预训练权重的路径，默认无\n",
    "\n",
    "    # 损失函数相关参数\n",
    "    parser.add_argument(\"--type_lambda\", type=float, default=1) # 类型损失的权重，默认为1\n",
    "    parser.add_argument(\"--sp_lambda\", type=float, default=5) # 支持性事实的损失权重，默认为5\n",
    "    parser.add_argument(\"--sp_threshold\", type=float, default=0.5) # 支持性事实的阈值，默认为0.5\n",
    "    parser.add_argument('--label_type_num', default=4, type=int)# 回答类型数目，包括yes/no/unknown/span，共4种\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    process_arguments(args) # 调用处理参数的函数\n",
    "    save_settings(args) # 调用保存设置的函数\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data_process\n",
    "这段代码主要包括两个部分：read_examples和convert_examples_to_features，它们共同完成了数据处理的任务，从原始的文本数据中提取出模型训练所需的格式化数据。\n",
    "# read_examples 函数\n",
    "* 功能: 该函数负责读取原始的数据文件，提取出每一个问题（Question）、相关文档（Document）、支持事实（Supporting Facts）、答案（Answer）等信息，并将它们封装成Example对象列表。这一步是数据预处理的第一阶段，主要目的是从复杂的原始数据中提取出对模型训练有用的结构化信息。\n",
    "* 原理: 函数通过遍历原始数据中的每一个案例，提取案例的关键信息。对于每个案例，它会记录问题ID、问题类型、文档中的句子、问题文本、支持事实的句子ID等信息。同时，它还会处理答案的位置，将答案文本在文档中的位置转换为基于单词的起始和结束位置。\n",
    "\n",
    "# convert_examples_to_features 函数\n",
    "* 功能: 该函数负责将Example对象转换为模型训练所需的特征格式，生成InputFeatures对象列表。这一步是数据预处理的第二阶段，目的是将上一步得到的结构化信息转换为模型可以直接处理的数值型特征。\n",
    "* 原理: 函数首先使用BERT Tokenizer对问题文本和文档文本进行分词，然后将文本转换为对应的词汇表索引（input IDs）。同时，它还会生成注意力掩码（input mask）和段落ID（segment IDs）等特征。对于答案位置和支持事实的句子位置，函数会根据分词结果对它们进行调整，确保位置信息与分词后的文本相匹配。\n",
    "\n",
    "# 数据预处理: \n",
    "这段代码通过两个步骤对原始数据进行了预处理，有效地将文本数据转换为了模型可以直接使用的特征，满足了项目对数据预处理的基本要求。\n",
    "* 灵活性和扩展性: 代码结构清晰，易于理解和修改。为本次实验要求的进一步探索要求应用新模型，只需对相应的部分进行小的调整即可。\n",
    "* 效率: 代码使用了gzip和pickle对处理后的数据进行压缩和序列化，提高了存储效率。同时，使用BertTokenizer进行高效的文本分词，并利用tqdm展示处理进度，使得我能够清晰的看到任务进度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import gzip\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "class Example(object):\n",
    "    # 初始化Example类，包含问题和文档的相关信息\n",
    "    def __init__(self,\n",
    "                 qas_id,\n",
    "                 qas_type,\n",
    "                 doc_tokens,\n",
    "                 question_text,\n",
    "                 sent_num,\n",
    "                 sent_names,\n",
    "                 sup_fact_id,\n",
    "                 para_start_end_position,\n",
    "                 sent_start_end_position,\n",
    "                 entity_start_end_position,\n",
    "                 orig_answer_text=None,\n",
    "                 start_position=None,\n",
    "                 end_position=None):\n",
    "        self.qas_id = qas_id\n",
    "        self.qas_type = qas_type\n",
    "        self.doc_tokens = doc_tokens\n",
    "        self.question_text = question_text\n",
    "        self.sent_num = sent_num\n",
    "        self.sent_names = sent_names\n",
    "        self.sup_fact_id = sup_fact_id\n",
    "        self.para_start_end_position = para_start_end_position\n",
    "        self.sent_start_end_position = sent_start_end_position\n",
    "        self.entity_start_end_position = entity_start_end_position\n",
    "        self.orig_answer_text = orig_answer_text\n",
    "        self.start_position = start_position\n",
    "        self.end_position = end_position\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"数据的一组特征。\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 qas_id,\n",
    "                 doc_tokens,\n",
    "                 doc_input_ids,\n",
    "                 doc_input_mask,\n",
    "                 doc_segment_ids,\n",
    "                 query_tokens,\n",
    "                 query_input_ids,\n",
    "                 query_input_mask,\n",
    "                 query_segment_ids,\n",
    "                 sent_spans,\n",
    "                 sup_fact_ids,\n",
    "                 ans_type,\n",
    "                 token_to_orig_map,\n",
    "                 start_position=None,\n",
    "                 end_position=None):\n",
    "\n",
    "        self.qas_id = qas_id\n",
    "        self.doc_tokens = doc_tokens\n",
    "        self.doc_input_ids = doc_input_ids\n",
    "        self.doc_input_mask = doc_input_mask\n",
    "        self.doc_segment_ids = doc_segment_ids\n",
    "\n",
    "        self.query_tokens = query_tokens\n",
    "        self.query_input_ids = query_input_ids\n",
    "        self.query_input_mask = query_input_mask\n",
    "        self.query_segment_ids = query_segment_ids\n",
    "\n",
    "        self.sent_spans = sent_spans\n",
    "        self.sup_fact_ids = sup_fact_ids\n",
    "        self.ans_type = ans_type\n",
    "        self.token_to_orig_map = token_to_orig_map\n",
    "\n",
    "        self.start_position = start_position\n",
    "        self.end_position = end_position\n",
    "\n",
    "\n",
    "def check_in_full_paras(answer, paras):\n",
    "    # 检查答案是否在所有段落中\n",
    "    full_doc = \"\"\n",
    "    for p in paras:\n",
    "        full_doc += \" \".join(p[1])\n",
    "    return answer in full_doc\n",
    "\n",
    "\n",
    "def read_examples(full_file):\n",
    "    # 读取示例数据\n",
    "    with open(full_file, 'r', encoding='utf-8') as reader:\n",
    "        full_data = json.load(reader)\n",
    "\n",
    "    def is_whitespace(c):\n",
    "        # 判断字符是否为空白字符\n",
    "        if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    cnt = 0\n",
    "    examples = []\n",
    "    for case in tqdm(full_data):\n",
    "        key = case['_id']\n",
    "        qas_type = \"\"  # case['type']\n",
    "        sup_facts = set([(sp[0], sp[1]) for sp in case['supporting_facts']])\n",
    "        sup_titles = set([sp[0] for sp in case['supporting_facts']])\n",
    "        orig_answer_text = case['answer']\n",
    "\n",
    "        sent_id = 0\n",
    "        doc_tokens = []\n",
    "        sent_names = []\n",
    "        sup_facts_sent_id = []\n",
    "        sent_start_end_position = []\n",
    "        para_start_end_position = []\n",
    "        entity_start_end_position = []\n",
    "        ans_start_position, ans_end_position = [], []\n",
    "\n",
    "        # 判断答案类型\n",
    "        JUDGE_FLAG = orig_answer_text == 'yes' or orig_answer_text == 'no' or orig_answer_text == 'unknown' or orig_answer_text == \"\"\n",
    "        FIND_FLAG = False\n",
    "\n",
    "        char_to_word_offset = []  # 累积所有句子的字符到单词的偏移\n",
    "        prev_is_whitespace = True\n",
    "\n",
    "        titles = set()\n",
    "        para_data = case['context']\n",
    "        for paragraph in para_data:\n",
    "            title = paragraph[0]\n",
    "            sents = paragraph[1]\n",
    "\n",
    "            titles.add(title)\n",
    "            is_gold_para = 1 if title in sup_titles else 0\n",
    "\n",
    "            para_start_position = len(doc_tokens)\n",
    "\n",
    "            for local_sent_id, sent in enumerate(sents):\n",
    "                if local_sent_id >= 100:\n",
    "                    break\n",
    "\n",
    "                # 确定支持事实的全局句子ID\n",
    "                local_sent_name = (title, local_sent_id)\n",
    "                sent_names.append(local_sent_name)\n",
    "                if local_sent_name in sup_facts:\n",
    "                    sup_facts_sent_id.append(sent_id)\n",
    "                sent_id += 1\n",
    "                sent = \" \".join(sent)\n",
    "                sent += \" \"\n",
    "\n",
    "                sent_start_word_id = len(doc_tokens)\n",
    "                sent_start_char_id = len(char_to_word_offset)\n",
    "\n",
    "                for c in sent:\n",
    "                    if is_whitespace(c):\n",
    "                        prev_is_whitespace = True\n",
    "                    else:\n",
    "                        if prev_is_whitespace:\n",
    "                            doc_tokens.append(c)\n",
    "                        else:\n",
    "                            doc_tokens[-1] += c\n",
    "                        prev_is_whitespace = False\n",
    "                    char_to_word_offset.append(len(doc_tokens) - 1)\n",
    "\n",
    "                sent_end_word_id = len(doc_tokens) - 1\n",
    "                sent_start_end_position.append((sent_start_word_id, sent_end_word_id))\n",
    "\n",
    "                # 答案字符位置\n",
    "                answer_offsets = []\n",
    "                offset = -1\n",
    "\n",
    "                tmp_answer = \" \".join(orig_answer_text)\n",
    "                while True:\n",
    "                    offset = sent.find(tmp_answer, offset + 1)\n",
    "                    if offset != -1:\n",
    "                        answer_offsets.append(offset)\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                if not JUDGE_FLAG and not FIND_FLAG and len(answer_offsets) > 0:\n",
    "                    FIND_FLAG = True\n",
    "                    for answer_offset in answer_offsets:\n",
    "                        start_char_position = sent_start_char_id + answer_offset\n",
    "                        end_char_position = start_char_position + len(tmp_answer) - 1\n",
    "\n",
    "                        ans_start_position.append(char_to_word_offset[start_char_position])\n",
    "                        ans_end_position.append(char_to_word_offset[end_char_position])\n",
    "\n",
    "                if len(doc_tokens) > 382:\n",
    "                    break\n",
    "            para_end_position = len(doc_tokens) - 1\n",
    "\n",
    "            para_start_end_position.append((para_start_position, para_end_position, title, is_gold_para))\n",
    "\n",
    "        if len(ans_end_position) > 1:\n",
    "            cnt += 1\n",
    "        if key < 10:\n",
    "            print(\"qid {}\".format(key))\n",
    "            print(\"qas type {}\".format(qas_type))\n",
    "            print(\"doc tokens {}\".format(doc_tokens))\n",
    "            print(\"question {}\".format(case['question']))\n",
    "            print(\"sent num {}\".format(sent_id + 1))\n",
    "            print(\"sup face id {}\".format(sup_facts_sent_id))\n",
    "            print(\"para_start_end_position {}\".format(para_start_end_position))\n",
    "            print(\"sent_start_end_position {}\".format(sent_start_end_position))\n",
    "            print(\"entity_start_end_position {}\".format(entity_start_end_position))\n",
    "            print(\"orig_answer_text {}\".format(orig_answer_text))\n",
    "            print(\"ans_start_position {}\".format(ans_start_position))\n",
    "            print(\"ans_end_position {}\".format(ans_end_position))\n",
    "\n",
    "        example = Example(\n",
    "            qas_id=key,\n",
    "            qas_type=qas_type,\n",
    "            doc_tokens=doc_tokens,\n",
    "            question_text=case['question'],\n",
    "            sent_num=sent_id + 1,\n",
    "            sent_names=sent_names,\n",
    "            sup_fact_id=sup_facts_sent_id,\n",
    "            para_start_end_position=para_start_end_position,\n",
    "            sent_start_end_position=sent_start_end_position,\n",
    "            entity_start_end_position=entity_start_end_position,\n",
    "            orig_answer_text=orig_answer_text,\n",
    "            start_position=ans_start_position,\n",
    "            end_position=ans_end_position)\n",
    "        examples.append(example)\n",
    "    return examples\n",
    "\n",
    "\n",
    "def convert_examples_to_features(examples, tokenizer, max_seq_length, max_query_length):\n",
    "    # 将示例转换为特征\n",
    "    features = []\n",
    "    failed = 0\n",
    "    for (example_index, example) in enumerate(tqdm(examples)):\n",
    "        if example.orig_answer_text == 'yes':\n",
    "            ans_type = 1\n",
    "        elif example.orig_answer_text == 'no':\n",
    "            ans_type = 2\n",
    "        elif example.orig_answer_text == 'unknown':\n",
    "            ans_type = 3\n",
    "        else:\n",
    "            ans_type = 0  # 统计答案类型\n",
    "\n",
    "        query_tokens = [\"[CLS]\"]\n",
    "        for token in example.question_text.split(' '):\n",
    "            query_tokens.extend(tokenizer.tokenize(token))\n",
    "        if len(query_tokens) > max_query_length - 1:\n",
    "            query_tokens = query_tokens[:max_query_length - 1]\n",
    "        query_tokens.append(\"[SEP]\")\n",
    "\n",
    "        sentence_spans = []\n",
    "        all_doc_tokens = []\n",
    "        orig_to_tok_index = []\n",
    "        orig_to_tok_back_index = []\n",
    "        tok_to_orig_index = [0] * len(query_tokens)\n",
    "\n",
    "        all_doc_tokens = [\"[CLS]\"]\n",
    "        for token in example.question_text.split(' '):\n",
    "            all_doc_tokens.extend(tokenizer.tokenize(token))\n",
    "        if len(all_doc_tokens) > max_query_length - 1:\n",
    "            all_doc_tokens = all_doc_tokens[:max_query_length - 1]\n",
    "        all_doc_tokens.append(\"[SEP]\")\n",
    "\n",
    "        for (i, token) in enumerate(example.doc_tokens):\n",
    "            orig_to_tok_index.append(len(all_doc_tokens))\n",
    "            sub_tokens = tokenizer.tokenize(token)\n",
    "            for sub_token in sub_tokens:\n",
    "                tok_to_orig_index.append(i)\n",
    "                all_doc_tokens.append(sub_token)\n",
    "            orig_to_tok_back_index.append(len(all_doc_tokens) - 1)\n",
    "\n",
    "        def relocate_tok_span(orig_start_position, orig_end_position, orig_text):\n",
    "            # 重新定位token化后的答案位置\n",
    "            if orig_start_position is None:\n",
    "                return 0, 0\n",
    "\n",
    "            tok_start_position = orig_to_tok_index[orig_start_position]\n",
    "            if orig_end_position < len(example.doc_tokens) - 1:\n",
    "                tok_end_position = orig_to_tok_index[orig_end_position + 1] - 1\n",
    "            else:\n",
    "                tok_end_position = len(all_doc_tokens) - 1\n",
    "\n",
    "            return _improve_answer_span(\n",
    "                all_doc_tokens, tok_start_position, tok_end_position, tokenizer, orig_text)\n",
    "\n",
    "        ans_start_position, ans_end_position = [], []\n",
    "        for ans_start_pos, ans_end_pos in zip(example.start_position, example.end_position):\n",
    "            s_pos, e_pos = relocate_tok_span(ans_start_pos, ans_end_pos, example.orig_answer_text)\n",
    "            ans_start_position.append(s_pos)\n",
    "            ans_end_position.append(e_pos)\n",
    "\n",
    "        for sent_span in example.sent_start_end_position:\n",
    "            if sent_span[0] >= len(orig_to_tok_index) or sent_span[0] >= sent_span[1]:\n",
    "                continue\n",
    "            sent_start_position = orig_to_tok_index[sent_span[0]]\n",
    "            sent_end_position = orig_to_tok_back_index[sent_span[1]]\n",
    "            sentence_spans.append((sent_start_position, sent_end_position))\n",
    "\n",
    "        all_doc_tokens = all_doc_tokens[:max_seq_length - 1] + [\"[SEP]\"]\n",
    "        doc_input_ids = tokenizer.convert_tokens_to_ids(all_doc_tokens)\n",
    "        query_input_ids = tokenizer.convert_tokens_to_ids(query_tokens)\n",
    "\n",
    "        doc_input_mask = [1] * len(doc_input_ids)\n",
    "        doc_segment_ids = [0] * len(query_input_ids) + [1] * (len(doc_input_ids) - len(query_input_ids))\n",
    "\n",
    "        while len(doc_input_ids) < max_seq_length:\n",
    "            doc_input_ids.append(0)\n",
    "            doc_input_mask.append(0)\n",
    "            doc_segment_ids.append(0)\n",
    "\n",
    "        query_input_mask = [1] * len(query_input_ids)\n",
    "        query_segment_ids = [0] * len(query_input_ids)\n",
    "\n",
    "        while len(query_input_ids) < max_query_length:\n",
    "            query_input_ids.append(0)\n",
    "            query_input_mask.append(0)\n",
    "            query_segment_ids.append(0)\n",
    "\n",
    "        assert len(doc_input_ids) == max_seq_length\n",
    "        assert len(doc_input_mask) == max_seq_length\n",
    "        assert len(doc_segment_ids) == max_seq_length\n",
    "        assert len(query_input_ids) == max_query_length\n",
    "        assert len(query_input_mask) == max_query_length\n",
    "        assert len(query_segment_ids) == max_query_length\n",
    "\n",
    "        sentence_spans = get_valid_spans(sentence_spans, max_seq_length)\n",
    "\n",
    "        sup_fact_ids = example.sup_fact_id\n",
    "        sent_num = len(sentence_spans)\n",
    "        sup_fact_ids = [sent_id for sent_id in sup_fact_ids if sent_id < sent_num]\n",
    "        if len(sup_fact_ids) != len(example.sup_fact_id):\n",
    "            failed += 1\n",
    "        if example.qas_id < 10:\n",
    "            print(\"qid {}\".format(example.qas_id))\n",
    "            print(\"all_doc_tokens {}\".format(all_doc_tokens))\n",
    "            print(\"doc_input_ids {}\".format(doc_input_ids))\n",
    "            print(\"doc_input_mask {}\".format(doc_input_mask))\n",
    "            print(\"doc_segment_ids {}\".format(doc_segment_ids))\n",
    "            print(\"query_tokens {}\".format(query_tokens))\n",
    "            print(\"query_input_ids {}\".format(query_input_ids))\n",
    "            print(\"query_input_mask {}\".format(query_input_mask))\n",
    "            print(\"query_segment_ids {}\".format(query_segment_ids))\n",
    "            print(\"sentence_spans {}\".format(sentence_spans))\n",
    "            print(\"sup_fact_ids {}\".format(sup_fact_ids))\n",
    "            print(\"ans_type {}\".format(ans_type))\n",
    "            print(\"tok_to_orig_index {}\".format(tok_to_orig_index))\n",
    "            print(\"ans_start_position {}\".format(ans_start_position))\n",
    "            print(\"ans_end_position {}\".format(ans_end_position))\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(qas_id=example.qas_id,\n",
    "                          doc_tokens=all_doc_tokens,\n",
    "                          doc_input_ids=doc_input_ids,\n",
    "                          doc_input_mask=doc_input_mask,\n",
    "                          doc_segment_ids=doc_segment_ids,\n",
    "                          query_tokens=query_tokens,\n",
    "                          query_input_ids=query_input_ids,\n",
    "                          query_input_mask=query_input_mask,\n",
    "                          query_segment_ids=query_segment_ids,\n",
    "                          sent_spans=sentence_spans,\n",
    "                          sup_fact_ids=sup_fact_ids,\n",
    "                          ans_type=ans_type,\n",
    "                          token_to_orig_map=tok_to_orig_index,\n",
    "                          start_position=ans_start_position,\n",
    "                          end_position=ans_end_position)\n",
    "        )\n",
    "    return features\n",
    "\n",
    "\n",
    "def _largest_valid_index(spans, limit):\n",
    "    # 获取最大的有效索引\n",
    "    for idx in range(len(spans)):\n",
    "        if spans[idx][1] >= limit:\n",
    "            return idx\n",
    "\n",
    "\n",
    "def get_valid_spans(spans, limit):\n",
    "    # 获取有效的跨度\n",
    "    new_spans = []\n",
    "    for span in spans:\n",
    "        if span[1] < limit:\n",
    "            new_spans.append(span)\n",
    "        else:\n",
    "            new_span = list(span)\n",
    "            new_span[1] = limit - 1\n",
    "            new_spans.append(tuple(new_span))\n",
    "            break\n",
    "    return new_spans\n",
    "\n",
    "\n",
    "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n",
    "                         orig_answer_text):\n",
    "    \"\"\"返回更好匹配标注答案的token化答案跨度。\"\"\"\n",
    "\n",
    "    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n",
    "\n",
    "    for new_start in range(input_start, input_end + 1):\n",
    "        for new_end in range(input_end, new_start - 1, -1):\n",
    "            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n",
    "            if text_span == tok_answer_text:\n",
    "                return new_start, new_end\n",
    "\n",
    "    return input_start, input_end\n",
    "\n",
    "\n",
    "# 主函数示例\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--full_data\", type=str, required=True, help=\"Path to the full data JSON file.\")\n",
    "    parser.add_argument(\"--tokenizer_path\", type=str, required=True, help=\"Path to pre-trained tokenizer\")\n",
    "    parser.add_argument(\"--example_output\", required=True, type=str, help=\"Path for the processed examples\")\n",
    "    parser.add_argument(\"--feature_output\", required=True, type=str, help=\"Path for the converted features\")\n",
    "    parser.add_argument(\"--max_seq_length\", default=512, type=int, help=\"Maximum sequence length of the input.\")\n",
    "    parser.add_argument(\"--batch_size\", default=15, type=int, help=\"Batch size for predictions.\")\n",
    "    parser.add_argument(\"--do_lower_case\", default=True, action='store_true', help=\"Lower case the input text. Should be True for uncased models.\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # 加载分词器\n",
    "    tokenizer = BertTokenizer.from_pretrained(args.tokenizer_path, do_lower_case=args.do_lower_case)\n",
    "\n",
    "    # 读取数据并转换为内部示例格式\n",
    "    examples = read_examples(args.full_data)\n",
    "    with gzip.open(args.example_output, 'wb') as fout:\n",
    "        pickle.dump(examples, fout)\n",
    "\n",
    "    # 将示例数据转为模型可以处理的特征\n",
    "    features = convert_examples_to_features(examples, tokenizer, max_seq_length=args.max_seq_length, max_query_length=50)\n",
    "    with gzip.open(args.feature_output, 'wb') as fout:\n",
    "        pickle.dump(features, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run_cail\n",
    "涵盖了从数据加载、模型初始化、训练、评估到结果保存的整个过程。\n",
    "\n",
    "* 主要组成部分\n",
    "* 参数和配置设置：通过set_config函数从config.py文件中导入训练和模型的配置参数。\n",
    "* 数据处理：使用DataHelper类处理数据，包括数据加载和预处理，将原始数据转换为模型可直接使用的格式。\n",
    "* 模型初始化：加载预训练的BERT模型，并基于此初始化项目特定的模型（BertSupportNet），同时设置优化器和学习率调度器。\n",
    "* 训练函数：定义了train_epoch和train_batch函数，用于控制模型的训练过程，包括数据批处理、损失计算、反向传播等。\n",
    "* 评估函数：定义了predict函数，用于在模型训练过程中或训练完成后进行模型评估，生成答案和支持性事实（sp）的预测结果，并保存到文件。\n",
    "* 损失函数：定义了计算损失的函数，考虑了答案位置预测、答案类型预测和支持性事实预测的损失。\n",
    "* 随机种子设置：为了实验的可复现性，提供了set_seed函数来设置随机种子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "from transformers import BertModel\n",
    "from transformers import BertConfig as BC\n",
    "\n",
    "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
    "from model.modeling import *\n",
    "from tools.utils import convert_to_tokens\n",
    "from tools.data_iterator_pack import IGNORE_INDEX\n",
    "import numpy as np\n",
    "import queue\n",
    "import random\n",
    "from config import set_config\n",
    "from tools.data_helper import DataHelper\n",
    "from data_process import InputFeatures, Example\n",
    "try:\n",
    "    from apex import amp\n",
    "except Exception:\n",
    "    print('Apex not imported!')\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def set_seed(args):\n",
    "    # 设置随机种子以确保结果的可重复性\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "\n",
    "def dispatch(context_encoding, context_mask, batch, device):\n",
    "    # 将context_encoding和context_mask移动到指定的GPU设备上\n",
    "    batch['context_encoding'] = context_encoding.cuda(device)\n",
    "    batch['context_mask'] = context_mask.float().cuda(device)\n",
    "    return batch\n",
    "\n",
    "\n",
    "def compute_loss(batch, start_logits, end_logits, type_logits, sp_logits, start_position, end_position):\n",
    "    # 计算损失函数，包括答案的起始和结束位置损失、问题类型损失和支持事实损失\n",
    "    loss1 = criterion(start_logits, batch['y1']) + criterion(end_logits, batch['y2'])\n",
    "    loss2 = args.type_lambda * criterion(type_logits, batch['q_type'])\n",
    "\n",
    "    sent_num_in_batch = batch[\"start_mapping\"].sum()\n",
    "    loss3 = args.sp_lambda * sp_loss_fct(sp_logits.view(-1), batch['is_support'].float().view(-1)).sum() / sent_num_in_batch\n",
    "    loss = loss1 + loss2 + loss3\n",
    "    return loss, loss1, loss2, loss3\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(model, dataloader, example_dict, feature_dict, prediction_file, need_sp_logit_file=False):\n",
    "    # 预测函数，不计算梯度\n",
    "    model.eval()\n",
    "    answer_dict = {}\n",
    "    sp_dict = {}\n",
    "    dataloader.refresh()\n",
    "    total_test_loss = [0] * 5\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        batch['context_mask'] = batch['context_mask'].float()\n",
    "        start_logits, end_logits, type_logits, sp_logits, start_position, end_position = model(batch)\n",
    "\n",
    "        loss_list = compute_loss(batch, start_logits, end_logits, type_logits, sp_logits, start_position, end_position)\n",
    "\n",
    "        for i, l in enumerate(loss_list):\n",
    "            if not isinstance(l, int):\n",
    "                total_test_loss[i] += l.item()\n",
    "\n",
    "        # 将预测结果转换为答案\n",
    "        answer_dict_ = convert_to_tokens(example_dict, feature_dict, batch['ids'], start_position.data.cpu().numpy().tolist(),\n",
    "                                         end_position.data.cpu().numpy().tolist(), np.argmax(type_logits.data.cpu().numpy(), 1))\n",
    "        answer_dict.update(answer_dict_)\n",
    "\n",
    "        # 预测支持事实\n",
    "        predict_support_np = torch.sigmoid(sp_logits).data.cpu().numpy()\n",
    "        for i in range(predict_support_np.shape[0]):\n",
    "            cur_sp_pred = []\n",
    "            cur_id = batch['ids'][i]\n",
    "\n",
    "            cur_sp_logit_pred = []  # 用于支持事实logit输出\n",
    "            for j in range(predict_support_np.shape[1]):\n",
    "                if j >= len(example_dict[cur_id].sent_names):\n",
    "                    break\n",
    "                if need_sp_logit_file:\n",
    "                    temp_title, temp_id = example_dict[cur_id].sent_names[j]\n",
    "                    cur_sp_logit_pred.append((temp_title, temp_id, predict_support_np[i, j]))\n",
    "                if predict_support_np[i, j] > args.sp_threshold:\n",
    "                    cur_sp_pred.append(example_dict[cur_id].sent_names[j])\n",
    "            sp_dict.update({cur_id: cur_sp_pred})\n",
    "\n",
    "    new_answer_dict = {}\n",
    "    for key, value in answer_dict.items():\n",
    "        new_answer_dict[key] = value.replace(\" \", \"\")\n",
    "    prediction = {'answer': new_answer_dict, 'sp': sp_dict}\n",
    "    with open(prediction_file, 'w', encoding='utf8') as f:\n",
    "        json.dump(prediction, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    for i, l in enumerate(total_test_loss):\n",
    "        print(\"Test Loss{}: {}\".format(i, l / len(dataloader)))\n",
    "    test_loss_record.append(sum(total_test_loss[:3]) / len(dataloader))\n",
    "\n",
    "\n",
    "def train_epoch(data_loader, model, predict_during_train=False):\n",
    "    # 训练一个epoch\n",
    "    model.train()\n",
    "    pbar = tqdm(total=len(data_loader))\n",
    "    epoch_len = len(data_loader)\n",
    "    step_count = 0\n",
    "    predict_step = epoch_len // 5\n",
    "    while not data_loader.empty():\n",
    "        step_count += 1\n",
    "        batch = next(iter(data_loader))\n",
    "        batch['context_mask'] = batch['context_mask'].float()\n",
    "        train_batch(model, batch)\n",
    "        del batch\n",
    "        if predict_during_train and (step_count % predict_step == 0):\n",
    "            predict(model, eval_dataset, dev_example_dict, dev_feature_dict,\n",
    "                    join(args.prediction_path, 'pred_seed_{}_epoch_{}_{}.json'.format(args.seed, epc, step_count)))\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            torch.save(model_to_save.state_dict(), join(args.checkpoint_path, \"ckpt_seed_{}_epoch_{}_{}.pth\".format(args.seed, epc, step_count)))\n",
    "            model.train()\n",
    "        pbar.update(1)\n",
    "\n",
    "    predict(model, eval_dataset, dev_example_dict, dev_feature_dict,\n",
    "            join(args.prediction_path, 'pred_seed_{}_epoch_{}_99999.json'.format(args.seed, epc)))\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "    torch.save(model_to_save.state_dict(), join(args.checkpoint_path, \"ckpt_seed_{}_epoch_{}_99999.pth\".format(args.seed, epc)))\n",
    "\n",
    "\n",
    "def train_batch(model, batch):\n",
    "    # 训练一个batch\n",
    "    global global_step, total_train_loss\n",
    "\n",
    "    start_logits, end_logits, type_logits, sp_logits, start_position, end_position = model(batch)\n",
    "    loss_list = compute_loss(batch, start_logits, end_logits, type_logits, sp_logits, start_position, end_position)\n",
    "    loss_list = list(loss_list)\n",
    "    if args.gradient_accumulation_steps > 1:\n",
    "        loss_list[0] = loss_list[0] / args.gradient_accumulation_steps\n",
    "    \n",
    "    if args.fp16:\n",
    "        with amp.scale_loss(loss_list[0], optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "    else:\n",
    "        loss_list[0].backward()\n",
    "\n",
    "    if (global_step + 1) % args.gradient_accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    global_step += 1\n",
    "\n",
    "    for i, l in enumerate(loss_list):\n",
    "        if not isinstance(l, int):\n",
    "            total_train_loss[i] += l.item()\n",
    "\n",
    "    if global_step % VERBOSE_STEP == 0:\n",
    "        print(\"{} -- In Epoch{}: \".format(args.name, epc))\n",
    "        for i, l in enumerate(total_train_loss):\n",
    "            print(\"Avg-LOSS{}/batch/step: {}\".format(i, l / VERBOSE_STEP))\n",
    "        total_train_loss = [0] * 5\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    args = set_config()\n",
    "\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "\n",
    "    if args.seed == 0:\n",
    "        args.seed = random.randint(0, 100)\n",
    "    set_seed(args)\n",
    "\n",
    "    helper = DataHelper(gz=True, config=args)\n",
    "    args.n_type = helper.n_type  # 2\n",
    "\n",
    "    # 设置数据集\n",
    "    Full_Loader = helper.train_loader\n",
    "    dev_example_dict = helper.dev_example_dict\n",
    "    dev_feature_dict = helper.dev_feature_dict\n",
    "    eval_dataset = helper.dev_loader\n",
    "\n",
    "    roberta_config = BC.from_pretrained(args.bert_model)\n",
    "    encoder = BertModel.from_pretrained(args.bert_model)\n",
    "    args.input_dim = roberta_config.hidden_size\n",
    "    model = BertSupportNet(config=args, encoder=encoder)\n",
    "    if args.trained_weight is not None:\n",
    "        model.load_state_dict(torch.load(args.trained_weight))\n",
    "    model.to('cuda')\n",
    "\n",
    "    # 初始化优化器和损失函数\n",
    "    lr = args.lr\n",
    "    t_total = len(Full_Loader) * args.epochs // args.gradient_accumulation_steps\n",
    "    warmup_steps = 0.1 * t_total\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, eps=1e-8)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps,\n",
    "                                                num_training_steps=t_total)\n",
    "    criterion = nn.CrossEntropyLoss(reduction='mean', ignore_index=IGNORE_INDEX)  # 交叉熵损失\n",
    "    binary_criterion = nn.BCEWithLogitsLoss(reduction='mean')  # 二分类损失\n",
    "    sp_loss_fct = nn.BCEWithLogitsLoss(reduction='none')  # 支持事实损失\n",
    "\n",
    "    if args.fp16:\n",
    "        import apex\n",
    "        apex.amp.register_half_function(torch, \"einsum\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level='O1')\n",
    "\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    model.train()\n",
    "\n",
    "    # 训练\n",
    "    global_step = epc = 0\n",
    "    total_train_loss = [0] * 5\n",
    "    test_loss_record = []\n",
    "    VERBOSE_STEP = args.verbose_step\n",
    "    while True:\n",
    "        if epc == args.epochs:  # 达到设定的训练轮数后退出\n",
    "            exit(0)\n",
    "        epc += 1\n",
    "\n",
    "        Loader = Full_Loader\n",
    "        Loader.refresh()\n",
    "\n",
    "        if epc > 2:\n",
    "            # 在训练过程中进行预测\n",
    "            train_epoch(Loader, model, predict_during_train=True)\n",
    "        else:\n",
    "            train_epoch(Loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling\n",
    "## SimplePredictionLayer 类\n",
    "* 功能：此类负责生成用于预测答案开始位置、结束位置、类型以及支持性事实（supporting facts）的逻辑回归（logits）。\n",
    "* 原理：它通过线性层（nn.Linear）对输入的特征进行变换，生成每个预测目标的逻辑回归。此外，它还使用了一个掩码（mask）机制来限制答案的可能位置，避免生成无效的答案区间。\n",
    "## BertSupportNet 类\n",
    "* 功能：该类是模型的主体，它集成了BERT模型（作为编码器）和SupportNet网络。\n",
    "* 原理：通过将输入文本传递给BERT编码器获取上下文编码，然后将这些编码和其他必要信息传递给SupportNet以生成最终的预测。\n",
    "## SupportNet 类\n",
    "* 功能：用于处理经过BERT编码后的上下文编码，并通过SimplePredictionLayer生成预测。\n",
    "* 原理：这一层主要是对SimplePredictionLayer的封装，以适应模型的整体架构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "class SimplePredictionLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(SimplePredictionLayer, self).__init__()\n",
    "        self.input_dim = config.input_dim\n",
    "\n",
    "        self.sp_linear = nn.Linear(self.input_dim, 1)\n",
    "        self.start_linear = nn.Linear(self.input_dim, 1)\n",
    "        self.end_linear = nn.Linear(self.input_dim, 1)\n",
    "\n",
    "        self.type_linear = nn.Linear(self.input_dim, config.label_type_num)   # yes/no/ans\n",
    "\n",
    "        self.cache_S = 0\n",
    "        self.cache_mask = None\n",
    "\n",
    "    def get_output_mask(self, outer):\n",
    "        # (batch, 512, 512)\n",
    "        S = outer.size(1)\n",
    "        if S <= self.cache_S:\n",
    "            return Variable(self.cache_mask[:S, :S], requires_grad=False)\n",
    "        self.cache_S = S\n",
    "        # triu 生成上三角矩阵，tril生成下三角矩阵，这个相当于生成了(512, 512)的矩阵表示开始-结束的位置，答案长度最长为15\n",
    "        np_mask = np.tril(np.triu(np.ones((S, S)), 0), 15)\n",
    "        self.cache_mask = outer.data.new(S, S).copy_(torch.from_numpy(np_mask))\n",
    "        return Variable(self.cache_mask, requires_grad=False)\n",
    "\n",
    "    def forward(self, batch, input_state):\n",
    "        query_mapping = batch['query_mapping']  # (batch, 512) 不一定是512，可能略小\n",
    "        context_mask = batch['context_mask']  # bert里实际有输入的位置\n",
    "        all_mapping = batch['all_mapping']  # (batch_size, 512, max_sent) 每个句子的token对应为1\n",
    "\n",
    "\n",
    "        start_logits = self.start_linear(input_state).squeeze(2) - 1e30 * (1 - context_mask)\n",
    "        end_logits = self.end_linear(input_state).squeeze(2) - 1e30 * (1 - context_mask)\n",
    "\n",
    "        sp_state = all_mapping.unsqueeze(3) * input_state.unsqueeze(2)  # N x sent x 512 x 300\n",
    "\n",
    "        sp_state = sp_state.max(1)[0]\n",
    "\n",
    "        sp_logits = self.sp_linear(sp_state)\n",
    "\n",
    "        type_state = torch.max(input_state, dim=1)[0]\n",
    "        type_logits = self.type_linear(type_state)\n",
    "\n",
    "        # 找结束位置用的开始和结束位置概率之和\n",
    "        # (batch, 512, 1) + (batch, 1, 512) -> (512, 512)\n",
    "        outer = start_logits[:, :, None] + end_logits[:, None]\n",
    "        outer_mask = self.get_output_mask(outer)\n",
    "        outer = outer - 1e30 * (1 - outer_mask[None].expand_as(outer))\n",
    "        if query_mapping is not None:   # 这个是query_mapping (batch, 512)\n",
    "            outer = outer - 1e30 * query_mapping[:, :, None]    # 不允许预测query的内容\n",
    "\n",
    "        # 这两句相当于找到了outer中最大值的i和j坐标\n",
    "        start_position = outer.max(dim=2)[0].max(dim=1)[1]\n",
    "        end_position = outer.max(dim=1)[0].max(dim=1)[1]\n",
    "\n",
    "        return start_logits, end_logits, type_logits, sp_logits.squeeze(2), start_position, end_position\n",
    "class BertSupportNet(nn.Module):\n",
    "    \"\"\"\n",
    "    joint train bert and graph fusion net\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, encoder):\n",
    "        super(BertSupportNet, self).__init__()\n",
    "        # self.bert_model = BertModel.from_pretrained(config.bert_model)\n",
    "        self.encoder = encoder\n",
    "        self.graph_fusion_net = SupportNet(config)\n",
    "\n",
    "    def forward(self, batch, debug=False):\n",
    "        doc_ids, doc_mask, segment_ids = batch['context_idxs'], batch['context_mask'], batch['segment_idxs']\n",
    "        # roberta不可以输入token_type_ids\n",
    "        all_doc_encoder_layers = self.encoder(input_ids=doc_ids,\n",
    "                                              token_type_ids=segment_ids,#可以注释\n",
    "                                              attention_mask=doc_mask)[0]\n",
    "        batch['context_encoding'] = all_doc_encoder_layers\n",
    "\n",
    "        return self.graph_fusion_net(batch)\n",
    "\n",
    "\n",
    "class SupportNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Packing Query Version\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(SupportNet, self).__init__()\n",
    "        self.config = config  # 就是args\n",
    "        # self.n_layers = config.n_layers  # 2\n",
    "        self.max_query_length = 50\n",
    "        self.prediction_layer = SimplePredictionLayer(config)\n",
    "\n",
    "    def forward(self, batch, debug=False):\n",
    "        context_encoding = batch['context_encoding']\n",
    "        predictions = self.prediction_layer(batch, context_encoding)\n",
    "\n",
    "        start_logits, end_logits, type_logits, sp_logits, start_position, end_position = predictions\n",
    "\n",
    "        return start_logits, end_logits, type_logits, sp_logits, start_position, end_position\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GFN\n",
    "此段代码定义了一个基于BERT的问答系统模型，包括两个主要部分：\n",
    "\n",
    "### SimplePredictionLayer\n",
    "一个简单的预测层，用于从输入状态中预测答案的起始位置、结束位置、问题类型以及支持事实。主要步骤包括：\n",
    "\n",
    "1. 定义线性层用于不同任务的预测。\n",
    "2. 计算起始和结束位置的logits，并应用掩码。\n",
    "3. 计算支持事实的状态和logits。\n",
    "4. 计算问题类型的logits。\n",
    "5. 根据logits确定答案的起始和结束位置。\n",
    "\n",
    "### BertSupportNet\n",
    "一个将BERT编码器和图融合网络结合的模型。主要步骤包括：\n",
    "\n",
    "1. 使用BERT编码器对输入文档进行编码。\n",
    "2. 将编码结果传递给图融合网络进行进一步处理。\n",
    "\n",
    "### SupportNet\n",
    "图融合网络，包含一个简单的预测层，用于最终的预测任务。\n",
    "\n",
    "这些类和方法共同构成了一个完整的问答系统模型，能够处理输入的上下文和问题，并预测答案的相关信息。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class SimplePredictionLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(SimplePredictionLayer, self).__init__()\n",
    "        self.input_dim = config.input_dim\n",
    "\n",
    "        # 定义线性层，用于预测起始位置、结束位置和支持事实\n",
    "        self.sp_linear = nn.Linear(self.input_dim, 1)\n",
    "        self.start_linear = nn.Linear(self.input_dim, 1)\n",
    "        self.end_linear = nn.Linear(self.input_dim, 1)\n",
    "\n",
    "        # 定义线性层，用于预测问题类型\n",
    "        self.type_linear = nn.Linear(self.input_dim, config.label_type_num)\n",
    "\n",
    "        self.cache_S = 0\n",
    "        self.cache_mask = None\n",
    "\n",
    "    def get_output_mask(self, outer):\n",
    "        # 获取输出掩码，确保只考虑合理的起始和结束位置\n",
    "        S = outer.size(1)\n",
    "        if S <= self.cache_S:\n",
    "            return Variable(self.cache_mask[:S, :S], requires_grad=False)\n",
    "        self.cache_S = S\n",
    "\n",
    "        np_mask = np.tril(np.triu(np.ones((S, S)), 0), 15)\n",
    "        self.cache_mask = outer.data.new(S, S).copy_(torch.from_numpy(np_mask))\n",
    "        return Variable(self.cache_mask, requires_grad=False)\n",
    "\n",
    "    def forward(self, batch, input_state):\n",
    "        query_mapping = batch['query_mapping']  # 查询映射\n",
    "        context_mask = batch['context_mask']  # 上下文掩码\n",
    "        all_mapping = batch['all_mapping']  # 全部映射\n",
    "\n",
    "        # 计算起始和结束位置的logits，并应用掩码\n",
    "        start_logits = self.start_linear(input_state).squeeze(2) - 1e30 * (1 - context_mask)\n",
    "        end_logits = self.end_linear(input_state).squeeze(2) - 1e30 * (1 - context_mask)\n",
    "\n",
    "        # 计算支持事实的状态\n",
    "        sp_state = all_mapping.unsqueeze(3) * input_state.unsqueeze(2)\n",
    "        sp_state = sp_state.max(1)[0]\n",
    "        sp_logits = self.sp_linear(sp_state)\n",
    "\n",
    "        # 计算问题类型的logits\n",
    "        type_state = torch.max(input_state, dim=1)[0]\n",
    "        type_logits = self.type_linear(type_state)\n",
    "\n",
    "        outer = start_logits[:, :, None] + end_logits[:, None]\n",
    "        outer_mask = self.get_output_mask(outer)\n",
    "        outer = outer - 1e30 * (1 - outer_mask[None].expand_as(outer))\n",
    "        if query_mapping is not None:\n",
    "            outer = outer - 1e30 * query_mapping[:, :, None]\n",
    "\n",
    "        # 获取起始和结束位置\n",
    "        start_position = outer.max(dim=2)[0].max(dim=1)[1]\n",
    "        end_position = outer.max(dim=1)[0].max(dim=1)[1]\n",
    "\n",
    "        return start_logits, end_logits, type_logits, sp_logits.squeeze(2), start_position, end_position\n",
    "\n",
    "class BertSupportNet(nn.Module):\n",
    "    \"\"\"\n",
    "    joint train bert and graph fusion net\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, encoder):\n",
    "        super(BertSupportNet, self).__init__()\n",
    "       \n",
    "        self.encoder = encoder\n",
    "        self.graph_fusion_net = SupportNet(config)\n",
    "\n",
    "    def forward(self, batch, debug=False):\n",
    "        # 从batch中获取输入数据\n",
    "        doc_ids, doc_mask, segment_ids = batch['context_idxs'], batch['context_mask'], batch['segment_idxs']\n",
    "\n",
    "        # 使用BERT编码器对输入进行编码\n",
    "        all_doc_encoder_layers = self.encoder(input_ids=doc_ids,\n",
    "                                              token_type_ids=segment_ids,\n",
    "                                              attention_mask=doc_mask)[0]\n",
    "        batch['context_encoding'] = all_doc_encoder_layers\n",
    "\n",
    "        # 将编码后的结果传给图融合网络\n",
    "        return self.graph_fusion_net(batch)\n",
    "\n",
    "\n",
    "class SupportNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Packing Query Version\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(SupportNet, self).__init__()\n",
    "        self.config = config  \n",
    "        \n",
    "        self.max_query_length = 50\n",
    "        self.prediction_layer = SimplePredictionLayer(config)\n",
    "\n",
    "    def forward(self, batch, debug=False):\n",
    "        # 获取上下文编码\n",
    "        context_encoding = batch['context_encoding']\n",
    "        # 使用预测层进行预测\n",
    "        predictions = self.prediction_layer(batch, context_encoding)\n",
    "\n",
    "        # 分别获取起始位置、结束位置、问题类型和支持事实的logits\n",
    "        start_logits, end_logits, type_logits, sp_logits, start_position, end_position = predictions\n",
    "\n",
    "        return start_logits, end_logits, type_logits, sp_logits, start_position, end_position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datahelper\n",
    "### DataHelper\n",
    "\n",
    "`DataHelper` 类是一个用于管理和加载数据的工具类，处理压缩文件（gzip格式）和序列化对象（pickle格式）。它主要包括以下几个部分：\n",
    "\n",
    "### 初始化方法 `__init__`\n",
    "- 接受参数 `gz` 和 `config`。\n",
    "- 根据 `gz` 参数设置文件后缀名为 `.pkl.gz` 或 `.pkl`。\n",
    "- 初始化多个数据占位符，如训练和开发的特征、示例和图数据。\n",
    "\n",
    "### 属性\n",
    "- `sent_limit`：句子长度限制，默认为100。\n",
    "- `entity_limit`：实体长度限制，默认为80。\n",
    "- `n_type`：类型数量，默认为2。\n",
    "- `train_feature_file` 和 `dev_feature_file`：分别返回训练和开发特征文件路径。\n",
    "- `train_example_file` 和 `dev_example_file`：分别返回训练和开发示例文件路径。\n",
    "\n",
    "### 方法\n",
    "- `get_feature_file(tag)`：根据标签返回特征文件路径。\n",
    "- `get_example_file(tag)`：根据标签返回示例文件路径。\n",
    "- `compress_pickle(pickle_file_name)`：压缩指定的pickle文件，并输出对象的简短描述。\n",
    "- `__load__(file)`：根据文件类型（json或pickle）加载数据。\n",
    "- `get_pickle_file(file_name)`：根据 `gz` 参数选择合适的方式打开pickle文件。\n",
    "- `__get_or_load__(name, file)`：如果属性为None，则从文件中加载数据并赋值给属性。\n",
    "\n",
    "### 特征数据\n",
    "- `train_features` 和 `dev_features`：分别获取训练和开发特征数据。\n",
    "- `train_feature_dict` 和 `dev_feature_dict`：分别返回训练和开发特征字典。\n",
    "\n",
    "### 示例数据\n",
    "- `train_examples` 和 `dev_examples`：分别获取训练和开发示例数据。\n",
    "- `train_example_dict` 和 `dev_example_dict`：分别返回训练和开发示例字典。\n",
    "\n",
    "### 数据加载\n",
    "- `load_dev()`：加载开发特征和示例数据。\n",
    "- `load_train()`：加载训练特征和示例数据。\n",
    "\n",
    "### 数据加载器\n",
    "- `dev_loader` 和 `train_loader`：分别返回开发和训练数据加载器。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join  # 导入os.path模块中的join函数，用于路径拼接\n",
    "import gzip  # 导入gzip模块，用于文件压缩和解压\n",
    "import pickle  # 导入pickle模块，用于对象序列化和反序列化\n",
    "import json  # 导入json模块，用于处理JSON数据\n",
    "from tqdm import tqdm  # 导入tqdm模块，用于显示进度条\n",
    "from tools.data_iterator_pack import DataIteratorPack  # 从tools.data_iterator_pack模块导入DataIteratorPack类\n",
    "\n",
    "\n",
    "class DataHelper:  # 定义一个DataHelper类\n",
    "    def __init__(self, gz=True, config=None):  # 类的初始化方法，接受gz和config两个参数\n",
    "        self.DataIterator = DataIteratorPack  # 将DataIteratorPack赋值给类的属性DataIterator\n",
    "        self.gz = gz  # 设置是否使用gzip压缩\n",
    "        self.suffix = '.pkl.gz' if gz else '.pkl'  # 根据是否压缩设置文件后缀\n",
    "\n",
    "        self.data_dir = '/home/mw/project/data'  # 数据目录路径\n",
    "\n",
    "        self.__train_features__ = None  # 训练特征数据占位符\n",
    "        self.__dev_features__ = None  # 开发特征数据占位符\n",
    "\n",
    "        self.__train_examples__ = None  # 训练示例数据占位符\n",
    "        self.__dev_examples__ = None  # 开发示例数据占位符\n",
    "\n",
    "        self.__train_graphs__ = None  # 训练图数据占位符\n",
    "        self.__dev_graphs__ = None  # 开发图数据占位符\n",
    "\n",
    "        self.__train_example_dict__ = None  # 训练示例字典占位符\n",
    "        self.__dev_example_dict__ = None  # 开发示例字典占位符\n",
    "\n",
    "        self.config = config  # 配置参数\n",
    "\n",
    "    @property  # 定义只读属性sent_limit\n",
    "    def sent_limit(self):   \n",
    "        return 100  # 返回句子长度限制为100\n",
    "\n",
    "    @property  # 定义只读属性entity_limit\n",
    "    def entity_limit(self):\n",
    "        return 80  # 返回实体长度限制为80\n",
    "\n",
    "    @property  # 定义只读属性n_type\n",
    "    def n_type(self):\n",
    "        return 2  # 返回类型数目为2\n",
    "\n",
    "    \n",
    "    def get_feature_file(self, tag):  # 根据标签获取特征文件路径\n",
    "        return join(self.data_dir, tag + '_feature' + self.suffix)\n",
    "\n",
    "    def get_example_file(self, tag):  # 根据标签获取示例文件路径\n",
    "        return join(self.data_dir, tag + '_example' + self.suffix)\n",
    "\n",
    "\n",
    "    @property  # 定义只读属性train_feature_file\n",
    "    def train_feature_file(self):\n",
    "        return self.get_feature_file('train')\n",
    "\n",
    "    @property  # 定义只读属性dev_feature_file\n",
    "    def dev_feature_file(self):\n",
    "        return self.get_feature_file('dev')\n",
    "\n",
    "    @property  # 定义只读属性train_example_file\n",
    "    def train_example_file(self):\n",
    "        return self.get_example_file('train')\n",
    "\n",
    "    @property  # 定义只读属性dev_example_file\n",
    "    def dev_example_file(self):\n",
    "        return self.get_example_file('dev')\n",
    "\n",
    "    @staticmethod  # 定义静态方法compress_pickle\n",
    "    def compress_pickle(pickle_file_name):  # 压缩pickle文件\n",
    "        def abbr(obj):  # 定义内部函数abbr，生成对象的简短描述\n",
    "            obj_str = str(obj)\n",
    "            if len(obj_str) > 100:\n",
    "                return obj_str[:20] + ' ... ' + obj_str[-20:]\n",
    "            else:\n",
    "                return obj_str\n",
    "\n",
    "        def get_obj_dict(pickle_obj):  # 定义内部函数get_obj_dict，获取对象的字典表示\n",
    "            if isinstance(pickle_obj, list):\n",
    "                obj = pickle_obj[0]\n",
    "            elif isinstance(pickle_obj, dict):\n",
    "                obj = list(pickle_obj.values())[0]\n",
    "            else:\n",
    "                obj = pickle_obj\n",
    "            if isinstance(obj, dict):\n",
    "                return obj\n",
    "            else:\n",
    "                return obj.__dict__\n",
    "\n",
    "        pickle_obj = pickle.load(open(pickle_file_name, 'rb'))  # 加载pickle对象\n",
    "\n",
    "        for k, v in get_obj_dict(pickle_obj).items():  # 打印对象字典的键和值的简短描述\n",
    "            print(k, abbr(v))\n",
    "        with gzip.open(pickle_file_name + '.gz', 'wb') as fout:  # 压缩并保存pickle对象\n",
    "            pickle.dump(pickle_obj, fout)\n",
    "        pickle_obj = pickle.load(gzip.open(pickle_file_name + '.gz', 'rb'))  # 重新加载压缩后的pickle对象\n",
    "        for k, v in get_obj_dict(pickle_obj).items():  # 打印重新加载后的对象字典的键和值的简短描述\n",
    "            print(k, abbr(v))\n",
    "\n",
    "    def __load__(self, file):  # 定义私有方法__load__，根据文件类型加载数据\n",
    "        if file.endswith('json'):\n",
    "            return json.load(open(file, 'r'))\n",
    "        with self.get_pickle_file(file) as fin:\n",
    "            print('loading', file)\n",
    "            return pickle.load(fin)\n",
    "\n",
    "    def get_pickle_file(self, file_name):  # 获取pickle文件对象，根据是否压缩选择打开方式\n",
    "        if self.gz:\n",
    "            return gzip.open(file_name, 'rb')\n",
    "        else:\n",
    "            return open(file_name, 'rb')\n",
    "\n",
    "    def __get_or_load__(self, name, file):  # 定义私有方法__get_or_load__，获取或加载数据\n",
    "        if getattr(self, name) is None:  # 如果属性为None，则加载数据\n",
    "            with self.get_pickle_file(file) as fin:\n",
    "                print('loading', file)\n",
    "                setattr(self, name, pickle.load(fin))\n",
    "\n",
    "        return getattr(self, name)  # 返回属性值\n",
    "\n",
    "    # Features 特征数据\n",
    "    @property\n",
    "    def train_features(self):  # 定义只读属性train_features，获取训练特征数据\n",
    "        return self.__get_or_load__('__train_features__', self.train_feature_file)\n",
    "\n",
    "    @property\n",
    "    def dev_features(self):  # 定义只读属性dev_features，获取开发特征数据\n",
    "        return self.__get_or_load__('__dev_features__', self.dev_feature_file)\n",
    "\n",
    "    # Examples 示例数据\n",
    "    @property\n",
    "    def train_examples(self):  # 定义只读属性train_examples，获取训练示例数据\n",
    "        return self.__get_or_load__('__train_examples__', self.train_example_file)\n",
    "\n",
    "    @property\n",
    "    def dev_examples(self):  # 定义只读属性dev_examples，获取开发示例数据\n",
    "        return self.__get_or_load__('__dev_examples__', self.dev_example_file)\n",
    "\n",
    "\n",
    "    # Example dict 示例字典\n",
    "    @property\n",
    "    def train_example_dict(self):  # 定义只读属性train_example_dict，获取训练示例字典\n",
    "        if self.__train_example_dict__ is None:\n",
    "            self.__train_example_dict__ = {e.qas_id: e for e in self.train_examples}\n",
    "        return self.__train_example_dict__\n",
    "\n",
    "    @property\n",
    "    def dev_example_dict(self):  # 定义只读属性dev_example_dict，获取开发示例字典\n",
    "        if self.__dev_example_dict__ is None:\n",
    "            self.__dev_example_dict__ = {e.qas_id: e for e in self.dev_examples}\n",
    "        return self.__dev_example_dict__\n",
    "\n",
    "    # Feature dict 特征字典\n",
    "    @property\n",
    "    def train_feature_dict(self):  # 定义只读属性train_feature_dict，获取训练特征字典\n",
    "        return {e.qas_id: e for e in self.train_features}\n",
    "\n",
    "    @property\n",
    "    def dev_feature_dict(self):  # 定义只读属性dev_feature_dict，获取开发特征字典\n",
    "        return {e.qas_id: e for e in self.dev_features}\n",
    "\n",
    "    # Load 加载数据\n",
    "    def load_dev(self):  # 加载开发数据\n",
    "        return self.dev_features, self.dev_example_dict  #, self.dev_graphs\n",
    "\n",
    "    def load_train(self):  # 加载训练数据\n",
    "        return self.train_features, self.train_example_dict  #, self.train_graphs\n",
    "\n",
    "\n",
    "\n",
    "    @property  # 定义只读属性dev_loader，获取开发数据加载器\n",
    "    def dev_loader(self):\n",
    "        return self.DataIterator(*self.load_dev(),   \n",
    "                                 bsz=self.config.eval_batch_size,\n",
    "                                 device='cuda:{}'.format(self.config.model_gpu),\n",
    "                                 sent_limit=self.sent_limit,  # 句子长度限制为25\n",
    "                                 entity_limit=self.entity_limit,\n",
    "                                 sequential=True,\n",
    "                                )\n",
    "\n",
    "    @property  # 定义只读属性train_loader，获取训练数据加载器\n",
    "    def train_loader(self):\n",
    "        return self.DataIterator(*self.load_train(),  # example, feature, graph\n",
    "                                 bsz=self.config.batch_size,\n",
    "                                 device='cuda:{}'.format(self.config.model_gpu),   \n",
    "                                 sent_limit=self.sent_limit,\n",
    "                                 entity_limit=self.entity_limit,\n",
    "                                 sequential=False\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data_iterator_pack\n",
    "\n",
    "\n",
    "### 初始化方法 `__init__`\n",
    "- 接受参数 `features`、`example_dict`、`bsz`、`device`、`sent_limit`、`entity_limit` 和 `sequential`。\n",
    "- 设置批处理大小、设备、特征列表、示例字典、句子长度限制和是否按顺序处理。\n",
    "- 如果不按顺序处理，则打乱特征列表。\n",
    "\n",
    "### 方法\n",
    "- `refresh()`：重置示例指针，并在非顺序处理的情况下重新打乱特征列表。\n",
    "- `empty()`：检查是否已处理完所有特征。\n",
    "- `__len__()`：返回迭代器的总批次数量。\n",
    "- `__iter__()`：迭代特征列表，生成批次数据并设置相关的输入和标签张量。\n",
    "\n",
    "### 数据生成过程\n",
    "- 初始化BERT输入张量和标签张量。\n",
    "- 在迭代过程中，根据批次大小和当前处理的位置生成当前批次。\n",
    "- 对当前批次按文档输入掩码总和排序，并设置相关映射和支持标志。\n",
    "- 根据答案类型设置标签张量。\n",
    "- 最后返回包含所有输入和标签的字典。\n",
    "\n",
    "这个类的主要功能是为模型提供预处理后的批次数据，并确保数据在每次迭代时正确设置和准备。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from numpy.random import shuffle\n",
    "\n",
    "IGNORE_INDEX = -100  # 忽略索引，通常用于标记无效的目标值\n",
    "\n",
    "\n",
    "class DataIteratorPack(object):\n",
    "    def __init__(self, features, example_dict, bsz, device, sent_limit, entity_limit,\n",
    "                 entity_type_dict=None, sequential=False):\n",
    "        self.bsz = bsz  # 批处理大小\n",
    "        self.device = device  # 设备（如cuda或cpu）\n",
    "        self.features = features  # 特征列表\n",
    "        self.example_dict = example_dict  # 示例字典\n",
    "        self.sequential = sequential  # 是否按顺序处理\n",
    "        self.sent_limit = sent_limit  # 句子长度限制\n",
    "        self.example_ptr = 0  # 当前处理的示例指针\n",
    "        if not sequential:\n",
    "            shuffle(self.features)  # 如果不按顺序处理，则打乱特征\n",
    "\n",
    "    def refresh(self):\n",
    "        self.example_ptr = 0  # 重置示例指针\n",
    "        if not self.sequential:\n",
    "            shuffle(self.features)  # 重新打乱特征\n",
    "\n",
    "    def empty(self):\n",
    "        return self.example_ptr >= len(self.features)  # 检查是否已处理完所有特征\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.features) / self.bsz))  # 返回迭代器的总批次数量\n",
    "\n",
    "    def __iter__(self):\n",
    "        # BERT输入张量\n",
    "        context_idxs = torch.LongTensor(self.bsz, 512)\n",
    "        context_mask = torch.LongTensor(self.bsz, 512)\n",
    "        segment_idxs = torch.LongTensor(self.bsz, 512)\n",
    "\n",
    "        query_mapping = torch.Tensor(self.bsz, 512).cuda(self.device)\n",
    "        start_mapping = torch.Tensor(self.bsz, self.sent_limit, 512).cuda(self.device)\n",
    "        all_mapping = torch.Tensor(self.bsz, 512, self.sent_limit).cuda(self.device)\n",
    "\n",
    "        # 标签张量\n",
    "        y1 = torch.LongTensor(self.bsz).cuda(self.device)\n",
    "        y2 = torch.LongTensor(self.bsz).cuda(self.device)\n",
    "        q_type = torch.LongTensor(self.bsz).cuda(self.device)\n",
    "        is_support = torch.FloatTensor(self.bsz, self.sent_limit).cuda(self.device)\n",
    "\n",
    "        while True:\n",
    "            if self.example_ptr >= len(self.features):\n",
    "                break  # 如果所有特征都处理完，则跳出循环\n",
    "            start_id = self.example_ptr  # 当前批处理的起始位置\n",
    "            cur_bsz = min(self.bsz, len(self.features) - start_id)  # 当前批处理的大小\n",
    "            cur_batch = self.features[start_id: start_id + cur_bsz]  # 当前批处理的特征\n",
    "            cur_batch.sort(key=lambda x: sum(x.doc_input_mask), reverse=True)  # 按文档输入掩码总和排序\n",
    "\n",
    "            ids = []\n",
    "            max_sent_cnt = 0\n",
    "            for mapping in [start_mapping, all_mapping, query_mapping]:\n",
    "                mapping.zero_()  # 将映射张量置零\n",
    "\n",
    "            is_support.fill_(0)  # 将支持标志张量置零\n",
    "\n",
    "            for i in range(len(cur_batch)):\n",
    "                case = cur_batch[i]  # 当前示例\n",
    "                context_idxs[i].copy_(torch.Tensor(case.doc_input_ids))\n",
    "                context_mask[i].copy_(torch.Tensor(case.doc_input_mask))\n",
    "                segment_idxs[i].copy_(torch.Tensor(case.doc_segment_ids))\n",
    "\n",
    "                for j in range(case.sent_spans[0][0] - 1):\n",
    "                    query_mapping[i, j] = 1  # 设置查询映射\n",
    "\n",
    "                # 根据答案类型设置标签\n",
    "                if case.ans_type == 0:\n",
    "                    if len(case.end_position) == 0:\n",
    "                        y1[i] = y2[i] = 0\n",
    "                    elif case.end_position[0] < 512:\n",
    "                        y1[i] = case.start_position[0]\n",
    "                        y2[i] = case.end_position[0]\n",
    "                    else:\n",
    "                        y1[i] = y2[i] = 0\n",
    "                    q_type[i] = 0\n",
    "                elif case.ans_type == 1:\n",
    "                    y1[i] = IGNORE_INDEX\n",
    "                    y2[i] = IGNORE_INDEX\n",
    "                    q_type[i] = 1\n",
    "                elif case.ans_type == 2:\n",
    "                    y1[i] = IGNORE_INDEX\n",
    "                    y2[i] = IGNORE_INDEX\n",
    "                    q_type[i] = 2\n",
    "                elif case.ans_type == 3:\n",
    "                    y1[i] = IGNORE_INDEX\n",
    "                    y2[i] = IGNORE_INDEX\n",
    "                    q_type[i] = 3\n",
    "\n",
    "                for j, sent_span in enumerate(case.sent_spans[:self.sent_limit]):\n",
    "                    is_sp_flag = j in case.sup_fact_ids  # 检查是否为支持句子\n",
    "                    start, end = sent_span\n",
    "                    if start < end:\n",
    "                        is_support[i, j] = int(is_sp_flag)  # 设置支持标志\n",
    "                        all_mapping[i, start:end + 1, j] = 1  # 设置全局映射\n",
    "                        start_mapping[i, j, start] = 1  # 设置起始映射\n",
    "\n",
    "                ids.append(case.qas_id)  # 添加问题ID\n",
    "                max_sent_cnt = max(max_sent_cnt, len(case.sent_spans))\n",
    "\n",
    "            input_lengths = (context_mask[:cur_bsz] > 0).long().sum(dim=1)\n",
    "            max_c_len = int(input_lengths.max())\n",
    "\n",
    "            self.example_ptr += cur_bsz  # 更新示例指针\n",
    "\n",
    "            yield {\n",
    "                'context_idxs': context_idxs[:cur_bsz, :max_c_len].contiguous(),\n",
    "                'context_mask': context_mask[:cur_bsz, :max_c_len].contiguous(),\n",
    "                'segment_idxs': segment_idxs[:cur_bsz, :max_c_len].contiguous(),\n",
    "                'query_mapping': query_mapping[:cur_bsz, :max_c_len].contiguous(),\n",
    "                'y1': y1[:cur_bsz],\n",
    "                'y2': y2[:cur_bsz],\n",
    "                'ids': ids,\n",
    "                'q_type': q_type[:cur_bsz],\n",
    "                'start_mapping': start_mapping[:cur_bsz, :max_sent_cnt, :max_c_len],\n",
    "                'all_mapping': all_mapping[:cur_bsz, :max_c_len, :max_sent_cnt],\n",
    "                'is_support': is_support[:cur_bsz, :max_sent_cnt].contiguous(),\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_final_text\n",
    "\n",
    "`get_final_text`函数将token化后的预测文本（`pred_text`）映射回原始文本（`orig_text`）。这个函数的目的是将预测的文本从经过WordPiece标记化的形式转换回原始的、更自然的文本表示。具体步骤如下：\n",
    "\n",
    "1. 使用`BasicTokenizer`进行token化。\n",
    "2. 找到`pred_text`在`orig_text`中的起始位置。\n",
    "3. 如果找不到，则返回原始文本。\n",
    "4. 去除空格，并创建字符对字符的映射。\n",
    "5. 确保去除空格后的文本长度相同。\n",
    "6. 使用字符对字符的映射将`pred_text`中的字符映射回`orig_text`。\n",
    "7. 返回映射后的最终文本。\n",
    "\n",
    "### convert_to_tokens\n",
    "\n",
    "`convert_to_tokens`函数将模型的输出转换为可读的答案文本。它接受参数`example`、`features`、`ids`、`y1`、`y2`和`q_type`，并生成一个包含问题ID和答案文本的字典。具体步骤如下：\n",
    "\n",
    "1. 初始化答案字典`answer_dict`。\n",
    "2. 遍历每个问题ID：\n",
    "   - 如果问题类型为0（文本答案），则根据预测的起始和结束位置从特征中提取对应的文档token。\n",
    "   - 使用`token_to_orig_map`将token映射回原始文档的位置。\n",
    "   - 将预测的token去除空格、去除WordPiece标记、清理空白，并映射回原始文本，得到最终答案文本。\n",
    "   - 如果问题类型为1，则答案为“yes”。\n",
    "   - 如果问题类型为2，则答案为“no”。\n",
    "   - 如果问题类型为3，则答案为“unknown”。\n",
    "3. 将答案文本添加到答案字典中并返回。\n",
    "\n",
    "这两个函数的共同作用是将模型的预测结果转换为原始文本形式，并生成最终的答案文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from transformers import BasicTokenizer\n",
    "import logging\n",
    "\n",
    "def get_final_text(pred_text, orig_text, do_lower_case, verbose_logging=False):\n",
    "    \"\"\"将标记化的预测文本映射回原始文本。\"\"\"\n",
    "\n",
    "    # 当我们创建数据时，我们跟踪了原始（空格标记）tokens与我们的WordPiece标记tokens之间的对齐。\n",
    "    # 因此，现在`orig_text`包含我们预测的span对应的原始文本的span。\n",
    "    # 然而，`orig_text`可能包含我们不希望出现在预测中的多余字符。\n",
    "    #\n",
    "    # 例如，假设：\n",
    "    #   pred_text = steve smith\n",
    "    #   orig_text = Steve Smith's\n",
    "    #\n",
    "    # 我们不希望返回`orig_text`，因为它包含多余的\"'s\"。\n",
    "    #\n",
    "    # 我们也不希望返回`pred_text`，因为它已经被规范化（SQuAD评估脚本也会进行标点符号剥离/小写处理，但我们的标记器进行了额外的规范化，如去除重音字符）。\n",
    "    #\n",
    "    # 我们真正想返回的是\"Steve Smith\"。\n",
    "    #\n",
    "    # 因此，我们必须在`pred_text`和`orig_text`之间应用一个半复杂的对齐启发式方法，以获得字符对字符的对齐。\n",
    "    # 这在某些情况下会失败，在这种情况下我们只返回`orig_text`。\n",
    "\n",
    "    def _strip_spaces(text):\n",
    "        \"\"\"去除空格并建立非空字符到原始字符的映射。\"\"\"\n",
    "        ns_chars = []  # 非空字符列表\n",
    "        ns_to_s_map = collections.OrderedDict()  # 非空字符到原始字符的映射\n",
    "        for (i, c) in enumerate(text):\n",
    "            if c == \" \":\n",
    "                continue\n",
    "            ns_to_s_map[len(ns_chars)] = i\n",
    "            ns_chars.append(c)\n",
    "        ns_text = \"\".join(ns_chars)\n",
    "        return (ns_text, ns_to_s_map)\n",
    "\n",
    "    tokenizer = BasicTokenizer(do_lower_case=do_lower_case)  # 初始化BasicTokenizer\n",
    "    tok_text = \" \".join(tokenizer.tokenize(orig_text))  # 标记化原始文本\n",
    "\n",
    "    start_position = tok_text.find(pred_text)  # 找到预测文本在标记化文本中的起始位置\n",
    "    if start_position == -1:  # 如果找不到\n",
    "        if verbose_logging:\n",
    "            print(\"无法在'%s'中找到文本：'%s'\" % (orig_text, pred_text))\n",
    "        return orig_text  # 返回原始文本\n",
    "    end_position = start_position + len(pred_text) - 1  # 计算结束位置\n",
    "    (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)  # 去除原始文本的空格\n",
    "    (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)  # 去除标记化文本的空格\n",
    "\n",
    "    if len(orig_ns_text) != len(tok_ns_text):  # 如果去除空格后的文本长度不同\n",
    "        if verbose_logging:\n",
    "            logging.info(\"去除空格后的长度不相等：'%s' vs '%s'\", orig_ns_text, tok_ns_text)\n",
    "        return orig_text  # 返回原始文本\n",
    "\n",
    "    # 投射字符到字符的对齐\n",
    "    tok_s_to_ns_map = {}\n",
    "    for (i, tok_index) in tok_ns_to_s_map.items():\n",
    "        tok_s_to_ns_map[tok_index] = i\n",
    "\n",
    "    orig_start_position = None\n",
    "    if start_position in tok_s_to_ns_map:\n",
    "        ns_start_position = tok_s_to_ns_map[start_position]\n",
    "        if ns_start_position in orig_ns_to_s_map:\n",
    "            orig_start_position = orig_ns_to_s_map[ns_start_position]\n",
    "\n",
    "    if orig_start_position is None:  # 如果起始位置无法映射\n",
    "        if verbose_logging:\n",
    "            print(\"无法映射起始位置\")\n",
    "        return orig_text\n",
    "\n",
    "    orig_end_position = None\n",
    "    if end_position in tok_s_to_ns_map:\n",
    "        ns_end_position = tok_s_to_ns_map[end_position]\n",
    "        if ns_end_position in orig_ns_to_s_map:\n",
    "            orig_end_position = orig_ns_to_s_map[ns_end_position]\n",
    "\n",
    "    if orig_end_position is None:  # 如果结束位置无法映射\n",
    "        if verbose_logging:\n",
    "            print(\"无法映射结束位置\")\n",
    "        return orig_text\n",
    "\n",
    "    output_text = orig_text[orig_start_position:(orig_end_position + 1)]  # 提取最终文本\n",
    "    return output_text\n",
    "\n",
    "def convert_to_tokens(example, features, ids, y1, y2, q_type):\n",
    "    \"\"\"将模型的输出转换为可读的答案文本。\"\"\"\n",
    "    answer_dict = dict()\n",
    "\n",
    "    for i, qid in enumerate(ids):  # 遍历每个问题ID\n",
    "        answer_text = ''\n",
    "        if q_type[i] == 0:  # 文本答案\n",
    "            doc_tokens = features[qid].doc_tokens  # 获取文档tokens\n",
    "            tok_tokens = doc_tokens[y1[i]: y2[i] + 1]  # 提取预测的tokens\n",
    "            tok_to_orig_map = features[qid].token_to_orig_map  # token到原始文本的映射\n",
    "            if y2[i] < len(tok_to_orig_map):  # 如果结束位置在映射范围内\n",
    "                orig_doc_start = tok_to_orig_map[y1[i]]\n",
    "                orig_doc_end = tok_to_orig_map[y2[i]]\n",
    "                orig_tokens = example[qid].doc_tokens[orig_doc_start:(orig_doc_end + 1)]  # 提取原始tokens\n",
    "                tok_text = \" \".join(tok_tokens)  # 拼接token文本\n",
    "\n",
    "                # 去除WordPiece标记\n",
    "                tok_text = tok_text.replace(\" ##\", \"\")\n",
    "                tok_text = tok_text.replace(\"##\", \"\")\n",
    "\n",
    "                # 清理空白\n",
    "                tok_text = tok_text.strip()\n",
    "                tok_text = \" \".join(tok_text.split())\n",
    "                orig_text = \" \".join(orig_tokens).strip('[,.;]')  # 去除多余字符\n",
    "\n",
    "                # 获取最终文本\n",
    "                final_text = get_final_text(tok_text, orig_text, do_lower_case=False, verbose_logging=False)\n",
    "                answer_text = final_text\n",
    "        elif q_type[i] == 1:  # 答案为“yes”\n",
    "            answer_text = 'yes'\n",
    "        elif q_type[i] == 2:  # 答案为“no”\n",
    "            answer_text = 'no'\n",
    "        elif q_type[i] == 3:  # 答案为“unknown”\n",
    "            answer_text = 'unknown'\n",
    "        answer_dict[qid] = answer_text  # 添加到答案字典中\n",
    "    return answer_dict  # 返回答案字典\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
