{"cells":[{"cell_type":"markdown","metadata":{"trusted":true,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"D89B0EAB542642F98900CCC4D4E49360","runtime":{"status":"default","execution_status":null,"is_visible":false},"scrolled":false,"notebookId":"66406df19ffe3bb7f164d64c"},"source":"## 欢迎进入 ModelWhale Notebook  \n\n这里你可以编写代码，文档  \n\n### 关于文件目录  \n\n\n**project**：project 目录是本项目的工作空间，可以把将项目运行有关的所有文件放在这里，目录中文件的增、删、改操作都会被保留  \n\n\n**input**：input 目录是数据集的挂载位置，所有挂载进项目的数据集都在这里，未挂载数据集时 input 目录被隐藏  \n\n\n**temp**：temp 目录是临时磁盘空间，训练或分析过程中产生的不必要文件可以存放在这里，目录中的文件不会保存  \n"},{"cell_type":"code","metadata":{"id":"810CF482643D4985AFA558B9F60AF78D","notebookId":"66406df19ffe3bb7f164d64c","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"source":"# 试试这个经典示例\nprint (\"hello ModelWhale\")","outputs":[{"output_type":"stream","name":"stdout","text":"hello ModelWhale\n"}],"execution_count":9},{"cell_type":"code","metadata":{"trusted":true,"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"4FC61B519BE24DD590C9913A15FBDA95","scrolled":false,"notebookId":"66406df19ffe3bb7f164d64c"},"source":"# 查看个人持久化工作区文件\n!ls /home/mw/project/","outputs":[{"output_type":"stream","name":"stdout","text":"cn-eng-train.txt\t    translated_test1.txt       translation_LSTM.txt\r\ncn-eng.txt\t\t    translated_test_BIGRU.txt  validation_set\r\ncn-eng-val.txt\t\t    translated_testGRU.txt\r\nformatted_translations.txt  translated_test.txt\r\n"}],"execution_count":10},{"cell_type":"code","metadata":{"trusted":true,"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"16BEDEF83DF24B65ABB0E52615CECDCB","scrolled":false,"notebookId":"66406df19ffe3bb7f164d64c"},"source":"# 查看当前挂载的数据集目录\n!ls /home/mw/input/","outputs":[{"output_type":"stream","name":"stdout","text":"cna8958\r\n"}],"execution_count":11},{"cell_type":"code","metadata":{"id":"70EEA4E7800349798EF24921DECC75D7","notebookId":"66406df19ffe3bb7f164d64c","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"","outputs":[{"output_type":"stream","name":"stdout","text":"Reading lines...\nRead 90000 sentence pairs\nTrimmed to 68898 sentence pairs\nIndexing words...\n['這是個很重要的會議 ', 'this is a very important meeting .']\n"},{"output_type":"error","ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (1x1500 and 2000x10)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[69], line 293\u001b[0m\n\u001b[1;32m    290\u001b[0m target_variable \u001b[38;5;241m=\u001b[39m training_pair[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    292\u001b[0m \u001b[38;5;66;03m# Run the train function\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_variable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_variable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# Keep track of loss\u001b[39;00m\n\u001b[1;32m    296\u001b[0m print_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n","Cell \u001b[0;32mIn[69], line 222\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# Without teacher forcing: use network's own prediction as the next input\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m di \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(target_length):\n\u001b[0;32m--> 222\u001b[0m         decoder_output, decoder_hidden, _ \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_hidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m         loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m criterion(decoder_output, target_variable[di])\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;66;03m# Get most likely word index (highest value) from output\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[69], line 171\u001b[0m, in \u001b[0;36mAttnDecoderLSTM.forward\u001b[0;34m(self, input, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    169\u001b[0m embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(embedded)\n\u001b[0;32m--> 171\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    172\u001b[0m attn_applied \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(attn_weights\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), encoder_outputs\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    174\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((embedded[\u001b[38;5;241m0\u001b[39m], attn_applied[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x1500 and 2000x10)"]}],"execution_count":69},{"cell_type":"code","metadata":{"id":"D3ED495512724535B6DD1DA5A287AE96","notebookId":"66406df19ffe3bb7f164d64c","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"import unicodedata\nimport string\nimport re\nimport random\nimport time\nimport math\n\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\n\nUSE_CUDA = True\n\nSOS_token = 0\nEOS_token = 1\n\n\n\nUNK_token = 2  # Index for the unknown word placeholder\n\n\n\n\n","outputs":[],"execution_count":12},{"cell_type":"code","metadata":{"id":"CD8F474043F446E1AEC63850B54A4083","notebookId":"66406df19ffe3bb7f164d64c","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"import jieba\n\nclass Lang:\n    def __init__(self, name):\n        self.name = name\n        self.word2index = {\"UNK\": UNK_token}\n        self.word2count = {\"UNK\": 0}\n        self.index2word = {0: \"SOS\", 1: \"EOS\", UNK_token: \"UNK\"}\n        self.n_words = 3  # Count SOS, EOS, and UNK\n\n    def index_words(self, sentence):\n        if self.name == 'cn':\n            for word in jieba.cut(sentence):\n                self.index_word(word)\n        else:\n            for word in sentence.split(' '):\n                self.index_word(word)\n    \n        # 将单词加入词典并更新索引\n    def index_word(self, word):\n        if word not in self.word2index:\n            self.word2index[word] = self.n_words\n            self.word2count[word] = 1\n            self.index2word[self.n_words] = word\n            self.n_words += 1\n        else:\n            self.word2count[word] += 1\n\n\n\n# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\ndef unicode_to_ascii(s):\n    return ''.join(\n        c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn'\n    )\n\n# Lowercase, trim, and remove non-letter characters\ndef normalize_string(s):\n    s = unicode_to_ascii(s.lower().strip())\n    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n    s = re.sub(r\"[^a-zA-Z\\u4e00-\\u9fa5.!?，。？]+\", r\" \", s)\n    return s\n\n\ndef read_langs(lang1, lang2, reverse=False):\n    print(\"Reading lines...\")\n\n    # Read the file and split into lines\n    lines = open('%s-%s.txt' % (lang1, lang2)).read().strip().split('\\n')\n\n    # Split every line into pairs and normalize\n    pairs = [[normalize_string(s) for s in l.split('\\t')] for l in lines]\n\n    # Reverse pairs, make Lang instances\n    if reverse:\n        pairs = [list(reversed(p)) for p in pairs]\n        input_lang = Lang(lang2)\n        output_lang = Lang(lang1)\n    else:\n        input_lang = Lang(lang1)\n        output_lang = Lang(lang2)\n\n    return input_lang, output_lang, pairs\n\nMAX_LENGTH = 10\n\ndef filter_pair(p):\n    return len(p[1].split(' ')) < MAX_LENGTH\n\ndef filter_pairs(pairs):\n    return [pair for pair in pairs if filter_pair(pair)]\n\n\ndef prepare_data(lang1_name, lang2_name, reverse=False):\n    input_lang, output_lang, pairs = read_langs(lang1_name, lang2_name, reverse)\n    print(\"Read %s sentence pairs\" % len(pairs))\n\n    pairs = filter_pairs(pairs)\n    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n\n    print(\"Indexing words...\")\n    for pair in pairs:\n        input_lang.index_words(pair[0])\n        output_lang.index_words(pair[1])\n\n    return input_lang, output_lang, pairs\n\n\ninput_lang, output_lang, pairs = prepare_data('cn', 'eng', False)\n\n# Print an example pair\nprint(random.choice(pairs))\n\n# Return a list of indexes, one for each word in the sentence\ndef indexes_from_sentence(lang, sentence):\n    if lang.name == 'cn':\n        return [lang.word2index.get(word, UNK_token) for word in sentence]\n    else:\n        return [lang.word2index.get(word, UNK_token) for word in sentence.split(' ')]\n        \ndef variable_from_sentence(lang, sentence):\n    indexes = indexes_from_sentence(lang, sentence)\n    indexes.append(EOS_token)\n    var = torch.LongTensor(indexes).view(-1, 1)\n    if USE_CUDA: var = var.cuda()\n    return var\n\ndef variables_from_pair(pair):\n    input_variable = variable_from_sentence(input_lang, pair[0])\n    target_variable = variable_from_sentence(output_lang, pair[1])\n    return (input_variable, target_variable)\n\n","outputs":[{"output_type":"stream","name":"stdout","text":"Reading lines...\nRead 90000 sentence pairs\nTrimmed to 68898 sentence pairs\nIndexing words...\n['他不喝咖啡。', 'he doesn t drink coffee .']\n"}],"execution_count":13},{"cell_type":"code","metadata":{"id":"8F6A347F1F8C4F27B5289042B502C93B","notebookId":"66406df19ffe3bb7f164d64c","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"","outputs":[],"execution_count":65},{"cell_type":"code","metadata":{"id":"34E2CB5C1A8E4CA5B7CCBB72D7FBC9B6","notebookId":"66406df19ffe3bb7f164d64c","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"class BioEncoderLSTM(nn.Module):\n    \"\"\"\n    双向LSTM编码器\n\n    Args:\n        input_size (int): 输入数据的大小\n        hidden_size (int): 隐藏层大小\n        n_layers (int): LSTM层数，默认为1\n    \"\"\"\n\n    def __init__(self, input_size, hidden_size, n_layers=1):\n        super(BioEncoderLSTM, self).__init__()\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n\n        self.embedding = nn.Embedding(input_size, hidden_size)\n        # 设置双向LSTM\n        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers, bidirectional=True)\n\n    def forward(self, word_inputs, hidden):\n        seq_len = len(word_inputs)\n        # 将输入的词嵌入表示\n        embedded = self.embedding(word_inputs).view(seq_len, 1, -1)\n        # 使用LSTM处理嵌入表示\n        output, hidden = self.lstm(embedded, hidden)\n        return output, hidden\n\n    def init_hidden(self):\n        # num_directions 设置为2\n        num_directions = 2\n        hidden = (torch.zeros(self.n_layers * num_directions, 1, self.hidden_size),\n                  torch.zeros(self.n_layers * num_directions, 1, self.hidden_size))\n        if USE_CUDA:\n            hidden = (hidden[0].cuda(), hidden[1].cuda())\n        return hidden\n\n\n\n","outputs":[],"execution_count":14},{"cell_type":"code","metadata":{"id":"7C2F126EEE4345BB891D9C35198C832C","notebookId":"66406df19ffe3bb7f164d64c","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"class DecoderLSTM(nn.Module):\n    \"\"\"\n    LSTM解码器\n\n    Args:\n        hidden_size (int): 隐藏层大小\n        output_size (int): 输出数据的大小\n        n_layers (int): LSTM层数，默认为1\n        dropout_p (float): dropout概率，默认为0.1\n    \"\"\"\n\n    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n        super(DecoderLSTM, self).__init__()\n\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.dropout_p = dropout_p\n\n        self.embedding = nn.Embedding(output_size, self.hidden_size)\n        self.rnn = nn.LSTM(self.hidden_size, self.hidden_size, n_layers, dropout=dropout_p)\n        self.out = nn.Linear(self.hidden_size, output_size)\n\n    def forward(self, word_input, last_hidden):\n        # 将输入的词嵌入表示\n        word_embedded = self.embedding(word_input).view(1, 1, -1)\n        # 使用LSTM处理嵌入表示\n        rnn_output, hidden = self.rnn(word_embedded, last_hidden)\n\n        rnn_output = rnn_output.squeeze(0)\n        # 输出通过线性层并进行log softmax\n        output = F.log_softmax(self.out(rnn_output), dim=1)\n\n        return output, hidden","outputs":[],"execution_count":15},{"cell_type":"code","metadata":{"id":"00FF7041ECF44C49ACCF2C8C41993884","notebookId":"66406df19ffe3bb7f164d64c","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"\n\n\n\nteacher_forcing_ratio = 0.5\nclip = 5.0\n\n\ndef train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion,\n          max_length=MAX_LENGTH):\n    # Zero gradients of both optimizers\n    encoder_optimizer.zero_grad()\n    decoder_optimizer.zero_grad()\n    loss = 0  # Added onto for each word\n\n    # Get size of input and target sentences\n    input_length = input_variable.size()[0]\n    target_length = target_variable.size()[0]\n\n    # Run words through encoder\n    encoder_hidden = encoder.init_hidden()\n    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n\n    # Prepare input and output variables\n    decoder_input = torch.LongTensor([[SOS_token]])\n    decoder_hidden = encoder_hidden  # Use last hidden state from encoder to start decoder\n    if USE_CUDA:\n        decoder_input = decoder_input.cuda()\n\n    # Choose whether to use teacher forcing\n    use_teacher_forcing = random.random() < teacher_forcing_ratio\n    if use_teacher_forcing:\n\n        # Teacher forcing: Use the ground-truth target as the next input\n        for di in range(target_length):\n            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n            loss += criterion(decoder_output, target_variable[di])\n            decoder_input = target_variable[di]  # Next target is next input\n\n    else:\n        # Without teacher forcing: use network's own prediction as the next input\n        for di in range(target_length):\n            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n            loss += criterion(decoder_output, target_variable[di])\n\n            # Get most likely word index (highest value) from output\n            topv, topi = decoder_output.data.topk(1)\n            ni = topi[0][0]\n\n            decoder_input = torch.LongTensor([[ni]])  # Chosen word is next input\n            if USE_CUDA: decoder_input = decoder_input.cuda()\n\n            # Stop at end of sentence (not necessary when using known targets)\n            if ni == EOS_token: break\n\n    # Backpropagation\n    loss.backward()\n    torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n    torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n    encoder_optimizer.step()\n    decoder_optimizer.step()\n\n    return loss.item() / target_length\n\n\n\n\n\ndef as_minutes(s):\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\ndef time_since(since, percent):\n    now = time.time()\n    s = now - since\n    es = s / (percent)\n    rs = es - s\n    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))\n\n\nhidden_size = 500\nn_layers = 2\ndropout_p = 0.05\n\n\n# Initialize models\nencoder = BioEncoderLSTM(input_lang.n_words, hidden_size, n_layers)\ndecoder = DecoderLSTM(hidden_size, output_lang.n_words, n_layers*2, dropout_p=dropout_p)\n\n\nif USE_CUDA:\n    encoder.cuda()\n    decoder.cuda()\n\n# Initialize optimizers and criterion\nlearning_rate = 0.0001\nencoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\ndecoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\ncriterion = nn.NLLLoss()\n\n\n# Configuring training\nn_epochs = 100000\nplot_every = 200\nprint_every = 1000\n\n# Keep track of time elapsed and running averages\nstart = time.time()\nplot_losses = []\nprint_loss_total = 0 # Reset every print_every\nplot_loss_total = 0 # Reset every plot_every\n\n\n# Begin!\nfor epoch in range(1, n_epochs + 1):\n\n    # Get training data for this cycle\n    training_pair = variables_from_pair(random.choice(pairs))\n    input_variable = training_pair[0]\n    target_variable = training_pair[1]\n\n    # Run the train function\n    loss = train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n\n    # Keep track of loss\n    print_loss_total += loss\n    plot_loss_total += loss\n\n    if epoch == 0: continue\n\n    if epoch % print_every == 0:\n        print_loss_avg = print_loss_total / print_every\n        print_loss_total = 0\n        print_summary = '%s (%d %d%%) %.4f' % (\n        time_since(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg)\n        print(print_summary)\n\n    if epoch % plot_every == 0:\n        plot_loss_avg = plot_loss_total / plot_every\n        plot_losses.append(plot_loss_avg)\n        plot_loss_total = 0\n\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport numpy as np\n%matplotlib inline\n\ndef show_plot(points):\n    plt.figure()\n    fig, ax = plt.subplots()\n    loc = ticker.MultipleLocator(base=0.2) # put ticks at regular intervals\n    ax.yaxis.set_major_locator(loc)\n    plt.plot(points)\n\nshow_plot(plot_losses)\n\ndef evaluate(sentence, max_length=MAX_LENGTH):\n    input_variable = variable_from_sentence(input_lang, sentence)\n    input_length = input_variable.size()[0]\n\n    # Run through encoder\n    encoder_hidden = encoder.init_hidden()\n    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n\n    # Create starting vectors for decoder\n    decoder_input = torch.LongTensor([[SOS_token]])  # SOS\n    if USE_CUDA:\n        decoder_input = decoder_input.cuda()\n\n    decoder_hidden = encoder_hidden\n\n    decoded_words = []\n    decoder_attentions = torch.zeros(max_length, max_length)\n\n    # Run through decoder\n    for di in range(max_length):\n        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n        # Choose top word from output\n        topv, topi = decoder_output.data.topk(1)\n        ni = topi[0][0]\n        if ni == EOS_token:\n            decoded_words.append('<EOS>')\n            break\n        else:\n            decoded_words.append(output_lang.index2word[ni.item()])\n\n        # Next input is chosen word\n        decoder_input = torch.LongTensor([[ni]])\n        if USE_CUDA: decoder_input = decoder_input.cuda()\n\n    return decoded_words\n\n\n\ndef evaluate_randomly():\n    pair = random.choice(pairs)\n\n    output_words = evaluate(pair[0])\n    output_sentence = ' '.join(output_words)\n\n    print('>', pair[0])\n    print('=', pair[1])\n    print('<', output_sentence)\n    print('')\n\nevaluate_randomly()\n\n\n","outputs":[{"output_type":"stream","name":"stderr","text":"/tmp/ipykernel_418/2388642189.py:54: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n  torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n/tmp/ipykernel_418/2388642189.py:55: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n  torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n"},{"output_type":"stream","name":"stdout","text":"0m 34s (- 56m 33s) (1000 1%) 5.3144\n1m 9s (- 56m 36s) (2000 2%) 4.9474\n1m 44s (- 56m 24s) (3000 3%) 4.7195\n2m 20s (- 56m 3s) (4000 4%) 4.6429\n2m 55s (- 55m 34s) (5000 5%) 4.5332\n3m 30s (- 55m 3s) (6000 6%) 4.4511\n4m 6s (- 54m 31s) (7000 7%) 4.4353\n4m 41s (- 54m 1s) (8000 8%) 4.2858\n5m 17s (- 53m 30s) (9000 9%) 4.2991\n5m 52s (- 52m 55s) (10000 10%) 4.1819\n6m 28s (- 52m 23s) (11000 11%) 4.1250\n7m 3s (- 51m 47s) (12000 12%) 4.1247\n7m 39s (- 51m 12s) (13000 13%) 4.0659\n8m 14s (- 50m 38s) (14000 14%) 4.0445\n8m 50s (- 50m 3s) (15000 15%) 3.9777\n9m 25s (- 49m 28s) (16000 16%) 3.9517\n10m 0s (- 48m 53s) (17000 17%) 3.9625\n10m 35s (- 48m 17s) (18000 18%) 3.9045\n11m 11s (- 47m 42s) (19000 19%) 3.8509\n11m 46s (- 47m 6s) (20000 20%) 3.8515\n12m 22s (- 46m 31s) (21000 21%) 3.7655\n12m 57s (- 45m 57s) (22000 22%) 3.8324\n13m 33s (- 45m 22s) (23000 23%) 3.7790\n14m 8s (- 44m 46s) (24000 24%) 3.7115\n14m 43s (- 44m 11s) (25000 25%) 3.7121\n15m 18s (- 43m 35s) (26000 26%) 3.6285\n15m 54s (- 43m 0s) (27000 27%) 3.6216\n16m 29s (- 42m 25s) (28000 28%) 3.5407\n17m 5s (- 41m 49s) (29000 28%) 3.6008\n17m 40s (- 41m 14s) (30000 30%) 3.5767\n18m 15s (- 40m 39s) (31000 31%) 3.5031\n18m 51s (- 40m 3s) (32000 32%) 3.5174\n19m 26s (- 39m 28s) (33000 33%) 3.4761\n20m 1s (- 38m 53s) (34000 34%) 3.4462\n20m 37s (- 38m 17s) (35000 35%) 3.4330\n21m 12s (- 37m 41s) (36000 36%) 3.4260\n21m 47s (- 37m 6s) (37000 37%) 3.3093\n22m 22s (- 36m 30s) (38000 38%) 3.4366\n22m 58s (- 35m 55s) (39000 39%) 3.3403\n23m 33s (- 35m 20s) (40000 40%) 3.3340\n24m 8s (- 34m 44s) (41000 41%) 3.2709\n24m 43s (- 34m 9s) (42000 42%) 3.2051\n25m 19s (- 33m 33s) (43000 43%) 3.2380\n25m 54s (- 32m 58s) (44000 44%) 3.1447\n26m 30s (- 32m 23s) (45000 45%) 3.2246\n27m 5s (- 31m 48s) (46000 46%) 3.1997\n27m 41s (- 31m 13s) (47000 47%) 3.1307\n28m 16s (- 30m 38s) (48000 48%) 3.1083\n28m 52s (- 30m 3s) (49000 49%) 3.1729\n29m 27s (- 29m 27s) (50000 50%) 3.0956\n30m 3s (- 28m 52s) (51000 51%) 3.0941\n30m 38s (- 28m 17s) (52000 52%) 3.0638\n31m 14s (- 27m 42s) (53000 53%) 3.1043\n31m 49s (- 27m 6s) (54000 54%) 3.0555\n32m 25s (- 26m 31s) (55000 55%) 3.0869\n33m 0s (- 25m 56s) (56000 56%) 3.0365\n33m 36s (- 25m 20s) (57000 56%) 3.0008\n34m 11s (- 24m 45s) (58000 57%) 3.0050\n34m 46s (- 24m 10s) (59000 59%) 3.0034\n35m 22s (- 23m 34s) (60000 60%) 2.8728\n35m 57s (- 22m 59s) (61000 61%) 2.9358\n36m 32s (- 22m 23s) (62000 62%) 3.0001\n37m 8s (- 21m 48s) (63000 63%) 2.8861\n37m 43s (- 21m 13s) (64000 64%) 2.9127\n38m 18s (- 20m 37s) (65000 65%) 2.9183\n38m 54s (- 20m 2s) (66000 66%) 2.8060\n39m 29s (- 19m 27s) (67000 67%) 2.8685\n40m 4s (- 18m 51s) (68000 68%) 2.8780\n40m 40s (- 18m 16s) (69000 69%) 2.8246\n41m 15s (- 17m 41s) (70000 70%) 2.7935\n41m 50s (- 17m 5s) (71000 71%) 2.7019\n42m 26s (- 16m 30s) (72000 72%) 2.8211\n43m 1s (- 15m 54s) (73000 73%) 2.7089\n43m 36s (- 15m 19s) (74000 74%) 2.7329\n44m 12s (- 14m 44s) (75000 75%) 2.6978\n44m 47s (- 14m 8s) (76000 76%) 2.7594\n45m 22s (- 13m 33s) (77000 77%) 2.7556\n45m 57s (- 12m 57s) (78000 78%) 2.6294\n46m 33s (- 12m 22s) (79000 79%) 2.6813\n47m 8s (- 11m 47s) (80000 80%) 2.6357\n47m 43s (- 11m 11s) (81000 81%) 2.6823\n48m 19s (- 10m 36s) (82000 82%) 2.5929\n48m 54s (- 10m 1s) (83000 83%) 2.5748\n49m 30s (- 9m 25s) (84000 84%) 2.5656\n50m 5s (- 8m 50s) (85000 85%) 2.5299\n50m 41s (- 8m 15s) (86000 86%) 2.5415\n51m 16s (- 7m 39s) (87000 87%) 2.5648\n51m 51s (- 7m 4s) (88000 88%) 2.4923\n52m 27s (- 6m 28s) (89000 89%) 2.5339\n53m 2s (- 5m 53s) (90000 90%) 2.5626\n53m 38s (- 5m 18s) (91000 91%) 2.4817\n54m 13s (- 4m 42s) (92000 92%) 2.5325\n54m 49s (- 4m 7s) (93000 93%) 2.4563\n55m 24s (- 3m 32s) (94000 94%) 2.4773\n56m 0s (- 2m 56s) (95000 95%) 2.5010\n56m 36s (- 2m 21s) (96000 96%) 2.3930\n57m 11s (- 1m 46s) (97000 97%) 2.4147\n57m 46s (- 1m 10s) (98000 98%) 2.4431\n58m 21s (- 0m 35s) (99000 99%) 2.3924\n58m 57s (- 0m 0s) (100000 100%) 2.4465\n> 你可以幫我寄這封信嗎 ?\n= will you mail this letter for me ?\n< could you mind this letter this letter ? <EOS>\n\n"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 0 Axes>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/upload/rt/00FF7041ECF44C49ACCF2C8C41993884/sgayzj45t5.png\">"},"metadata":{}}],"execution_count":16},{"cell_type":"code","metadata":{"id":"585E40B2A35C4427AF86361189EDB98C","notebookId":"66406df19ffe3bb7f164d64c","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"def sample_test_dataset(size=100):\n    with open('cn-eng-test.txt', 'w+') as f:\n        f.write('\\n'.join(['\\t'.join(pair) for pair in random.sample(pairs, k=size)]))\n\nsample_test_dataset()\n","outputs":[],"execution_count":17},{"cell_type":"code","metadata":{"id":"F15202ACAA414A2B9C611A61EDC2ACE7","notebookId":"66406df19ffe3bb7f164d64c","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"import collections\nfrom torchtext.data.metrics import bleu_score\n\n\n# 读取测试数据集\nwith open('/home/mw/project/cn-eng-test.txt') as f:\n    lines = f.read().strip().split('\\n')\n    \n    test_pairs = [[normalize_string(s) for s in l.split('\\t')] for l in lines]\n\n\n\ntest_pairs_dict = collections.defaultdict(lambda : [])\n\nfor pair in test_pairs:\n    test_pairs_dict[pair[0]].append(pair[1].split(' '))\n\n\ndef evaluate_bleu_score():\n    candicates = []\n    references = []\n\n    for i, pair in enumerate(test_pairs_dict.items(), start=1):\n        candicate = evaluate(pair[0])\n        if candicate[-1] == '<EOS>':\n            candicate.pop(-1)\n        candicates.append(candicate)\n        references.append(pair[1])\n    \n    score = bleu_score(candicates, references)\n    return score\n\nprint('test dataset bleu score: %s' % evaluate_bleu_score())\n","outputs":[{"output_type":"stream","name":"stdout","text":"test dataset bleu score: 0.15587388613826067\n"}],"execution_count":20},{"cell_type":"code","metadata":{"id":"A73F65DE29384C8B8DCBFDDE8414D946","notebookId":"66406df19ffe3bb7f164d64c","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"def translate_test_file(test_file_path, output_file_path):\n    # 读取测试文件\n    with open(test_file_path, 'r', encoding='utf-8') as test_file:\n        lines = test_file.readlines()\n    \n    # 准备输出文件\n    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n        # 对每一行（句子）进行翻译\n        for line in lines:\n            line = line.strip()  # 去除可能的前后空格\n            if not line:  # 跳过空行\n                continue\n            # 使用模型进行翻译\n            output_words = evaluate(normalize_string(line))\n            output_sentence = ' '.join(output_words[:-1])  # 去除<EOS>标记\n            # 写入原句和翻译结果\n            output_file.write(f'{line}\\n{output_sentence}\\n\\n')\n            \n# 调用函数，传入测试文件路径和输出文件路径\ntranslate_test_file('/home/mw/input/cna8958/test.txt', '/home/mw/project/translated_test_LSTM.txt')","outputs":[],"execution_count":23}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","version":"3.5.2","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":0}