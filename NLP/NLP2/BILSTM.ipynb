{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D89B0EAB542642F98900CCC4D4E49360",
    "jupyter": {},
    "notebookId": "66406df19ffe3bb7f164d64c",
    "runtime": {
     "execution_status": null,
     "is_visible": false,
     "status": "default"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "trusted": true
   },
   "source": [
    "## 欢迎进入 ModelWhale Notebook  \n",
    "\n",
    "这里你可以编写代码，文档  \n",
    "\n",
    "### 关于文件目录  \n",
    "\n",
    "\n",
    "**project**：project 目录是本项目的工作空间，可以把将项目运行有关的所有文件放在这里，目录中文件的增、删、改操作都会被保留  \n",
    "\n",
    "\n",
    "**input**：input 目录是数据集的挂载位置，所有挂载进项目的数据集都在这里，未挂载数据集时 input 目录被隐藏  \n",
    "\n",
    "\n",
    "**temp**：temp 目录是临时磁盘空间，训练或分析过程中产生的不必要文件可以存放在这里，目录中的文件不会保存  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "id": "810CF482643D4985AFA558B9F60AF78D",
    "jupyter": {},
    "notebookId": "66406df19ffe3bb7f164d64c",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello ModelWhale\n"
     ]
    }
   ],
   "source": [
    "# 试试这个经典示例\n",
    "print (\"hello ModelWhale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "id": "4FC61B519BE24DD590C9913A15FBDA95",
    "jupyter": {},
    "notebookId": "66406df19ffe3bb7f164d64c",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cn-eng-train.txt\t    translated_test1.txt       translation_LSTM.txt\r\n",
      "cn-eng.txt\t\t    translated_test_BIGRU.txt  validation_set\r\n",
      "cn-eng-val.txt\t\t    translated_testGRU.txt\r\n",
      "formatted_translations.txt  translated_test.txt\r\n"
     ]
    }
   ],
   "source": [
    "# 查看个人持久化工作区文件\n",
    "!ls /home/mw/project/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "id": "16BEDEF83DF24B65ABB0E52615CECDCB",
    "jupyter": {},
    "notebookId": "66406df19ffe3bb7f164d64c",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cna8958\r\n"
     ]
    }
   ],
   "source": [
    "# 查看当前挂载的数据集目录\n",
    "!ls /home/mw/input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "70EEA4E7800349798EF24921DECC75D7",
    "jupyter": {},
    "notebookId": "66406df19ffe3bb7f164d64c",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "id": "D3ED495512724535B6DD1DA5A287AE96",
    "jupyter": {},
    "notebookId": "66406df19ffe3bb7f164d64c",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "USE_CUDA = True\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "\n",
    "UNK_token = 2  # Index for the unknown word placeholder\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "id": "CD8F474043F446E1AEC63850B54A4083",
    "jupyter": {},
    "notebookId": "66406df19ffe3bb7f164d64c",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 90000 sentence pairs\n",
      "Trimmed to 68898 sentence pairs\n",
      "Indexing words...\n",
      "['他不喝咖啡。', 'he doesn t drink coffee .']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {\"UNK\": UNK_token}\n",
    "        self.word2count = {\"UNK\": 0}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", UNK_token: \"UNK\"}\n",
    "        self.n_words = 3  # Count SOS, EOS, and UNK\n",
    "\n",
    "    def index_words(self, sentence):\n",
    "        if self.name == 'cn':\n",
    "            for word in jieba.cut(sentence):\n",
    "                self.index_word(word)\n",
    "        else:\n",
    "            for word in sentence.split(' '):\n",
    "                self.index_word(word)\n",
    "    \n",
    "        # 将单词加入词典并更新索引\n",
    "    def index_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z\\u4e00-\\u9fa5.!?，。？]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def read_langs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('%s-%s.txt' % (lang1, lang2)).read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalize_string(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "def filter_pair(p):\n",
    "    return len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "def filter_pairs(pairs):\n",
    "    return [pair for pair in pairs if filter_pair(pair)]\n",
    "\n",
    "\n",
    "def prepare_data(lang1_name, lang2_name, reverse=False):\n",
    "    input_lang, output_lang, pairs = read_langs(lang1_name, lang2_name, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "\n",
    "    pairs = filter_pairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "\n",
    "    print(\"Indexing words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.index_words(pair[0])\n",
    "        output_lang.index_words(pair[1])\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepare_data('cn', 'eng', False)\n",
    "\n",
    "# Print an example pair\n",
    "print(random.choice(pairs))\n",
    "\n",
    "# Return a list of indexes, one for each word in the sentence\n",
    "def indexes_from_sentence(lang, sentence):\n",
    "    if lang.name == 'cn':\n",
    "        return [lang.word2index.get(word, UNK_token) for word in sentence]\n",
    "    else:\n",
    "        return [lang.word2index.get(word, UNK_token) for word in sentence.split(' ')]\n",
    "        \n",
    "def variable_from_sentence(lang, sentence):\n",
    "    indexes = indexes_from_sentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    var = torch.LongTensor(indexes).view(-1, 1)\n",
    "    if USE_CUDA: var = var.cuda()\n",
    "    return var\n",
    "\n",
    "def variables_from_pair(pair):\n",
    "    input_variable = variable_from_sentence(input_lang, pair[0])\n",
    "    target_variable = variable_from_sentence(output_lang, pair[1])\n",
    "    return (input_variable, target_variable)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "id": "8F6A347F1F8C4F27B5289042B502C93B",
    "jupyter": {},
    "notebookId": "66406df19ffe3bb7f164d64c",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "id": "34E2CB5C1A8E4CA5B7CCBB72D7FBC9B6",
    "jupyter": {},
    "notebookId": "66406df19ffe3bb7f164d64c",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BioEncoderLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    双向LSTM编码器\n",
    "\n",
    "    Args:\n",
    "        input_size (int): 输入数据的大小\n",
    "        hidden_size (int): 隐藏层大小\n",
    "        n_layers (int): LSTM层数，默认为1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(BioEncoderLSTM, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        # 设置双向LSTM\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers, bidirectional=True)\n",
    "\n",
    "    def forward(self, word_inputs, hidden):\n",
    "        seq_len = len(word_inputs)\n",
    "        # 将输入的词嵌入表示\n",
    "        embedded = self.embedding(word_inputs).view(seq_len, 1, -1)\n",
    "        # 使用LSTM处理嵌入表示\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # num_directions 设置为2\n",
    "        num_directions = 2\n",
    "        hidden = (torch.zeros(self.n_layers * num_directions, 1, self.hidden_size),\n",
    "                  torch.zeros(self.n_layers * num_directions, 1, self.hidden_size))\n",
    "        if USE_CUDA:\n",
    "            hidden = (hidden[0].cuda(), hidden[1].cuda())\n",
    "        return hidden\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "id": "7C2F126EEE4345BB891D9C35198C832C",
    "jupyter": {},
    "notebookId": "66406df19ffe3bb7f164d64c",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM解码器\n",
    "\n",
    "    Args:\n",
    "        hidden_size (int): 隐藏层大小\n",
    "        output_size (int): 输出数据的大小\n",
    "        n_layers (int): LSTM层数，默认为1\n",
    "        dropout_p (float): dropout概率，默认为0.1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, self.hidden_size)\n",
    "        self.rnn = nn.LSTM(self.hidden_size, self.hidden_size, n_layers, dropout=dropout_p)\n",
    "        self.out = nn.Linear(self.hidden_size, output_size)\n",
    "\n",
    "    def forward(self, word_input, last_hidden):\n",
    "        # 将输入的词嵌入表示\n",
    "        word_embedded = self.embedding(word_input).view(1, 1, -1)\n",
    "        # 使用LSTM处理嵌入表示\n",
    "        rnn_output, hidden = self.rnn(word_embedded, last_hidden)\n",
    "\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        # 输出通过线性层并进行log softmax\n",
    "        output = F.log_softmax(self.out(rnn_output), dim=1)\n",
    "\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "id": "00FF7041ECF44C49ACCF2C8C41993884",
    "jupyter": {},
    "notebookId": "66406df19ffe3bb7f164d64c",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_418/2388642189.py:54: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
      "/tmp/ipykernel_418/2388642189.py:55: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 34s (- 56m 33s) (1000 1%) 5.3144\n",
      "1m 9s (- 56m 36s) (2000 2%) 4.9474\n",
      "1m 44s (- 56m 24s) (3000 3%) 4.7195\n",
      "2m 20s (- 56m 3s) (4000 4%) 4.6429\n",
      "2m 55s (- 55m 34s) (5000 5%) 4.5332\n",
      "3m 30s (- 55m 3s) (6000 6%) 4.4511\n",
      "4m 6s (- 54m 31s) (7000 7%) 4.4353\n",
      "4m 41s (- 54m 1s) (8000 8%) 4.2858\n",
      "5m 17s (- 53m 30s) (9000 9%) 4.2991\n",
      "5m 52s (- 52m 55s) (10000 10%) 4.1819\n",
      "6m 28s (- 52m 23s) (11000 11%) 4.1250\n",
      "7m 3s (- 51m 47s) (12000 12%) 4.1247\n",
      "7m 39s (- 51m 12s) (13000 13%) 4.0659\n",
      "8m 14s (- 50m 38s) (14000 14%) 4.0445\n",
      "8m 50s (- 50m 3s) (15000 15%) 3.9777\n",
      "9m 25s (- 49m 28s) (16000 16%) 3.9517\n",
      "10m 0s (- 48m 53s) (17000 17%) 3.9625\n",
      "10m 35s (- 48m 17s) (18000 18%) 3.9045\n",
      "11m 11s (- 47m 42s) (19000 19%) 3.8509\n",
      "11m 46s (- 47m 6s) (20000 20%) 3.8515\n",
      "12m 22s (- 46m 31s) (21000 21%) 3.7655\n",
      "12m 57s (- 45m 57s) (22000 22%) 3.8324\n",
      "13m 33s (- 45m 22s) (23000 23%) 3.7790\n",
      "14m 8s (- 44m 46s) (24000 24%) 3.7115\n",
      "14m 43s (- 44m 11s) (25000 25%) 3.7121\n",
      "15m 18s (- 43m 35s) (26000 26%) 3.6285\n",
      "15m 54s (- 43m 0s) (27000 27%) 3.6216\n",
      "16m 29s (- 42m 25s) (28000 28%) 3.5407\n",
      "17m 5s (- 41m 49s) (29000 28%) 3.6008\n",
      "17m 40s (- 41m 14s) (30000 30%) 3.5767\n",
      "18m 15s (- 40m 39s) (31000 31%) 3.5031\n",
      "18m 51s (- 40m 3s) (32000 32%) 3.5174\n",
      "19m 26s (- 39m 28s) (33000 33%) 3.4761\n",
      "20m 1s (- 38m 53s) (34000 34%) 3.4462\n",
      "20m 37s (- 38m 17s) (35000 35%) 3.4330\n",
      "21m 12s (- 37m 41s) (36000 36%) 3.4260\n",
      "21m 47s (- 37m 6s) (37000 37%) 3.3093\n",
      "22m 22s (- 36m 30s) (38000 38%) 3.4366\n",
      "22m 58s (- 35m 55s) (39000 39%) 3.3403\n",
      "23m 33s (- 35m 20s) (40000 40%) 3.3340\n",
      "24m 8s (- 34m 44s) (41000 41%) 3.2709\n",
      "24m 43s (- 34m 9s) (42000 42%) 3.2051\n",
      "25m 19s (- 33m 33s) (43000 43%) 3.2380\n",
      "25m 54s (- 32m 58s) (44000 44%) 3.1447\n",
      "26m 30s (- 32m 23s) (45000 45%) 3.2246\n",
      "27m 5s (- 31m 48s) (46000 46%) 3.1997\n",
      "27m 41s (- 31m 13s) (47000 47%) 3.1307\n",
      "28m 16s (- 30m 38s) (48000 48%) 3.1083\n",
      "28m 52s (- 30m 3s) (49000 49%) 3.1729\n",
      "29m 27s (- 29m 27s) (50000 50%) 3.0956\n",
      "30m 3s (- 28m 52s) (51000 51%) 3.0941\n",
      "30m 38s (- 28m 17s) (52000 52%) 3.0638\n",
      "31m 14s (- 27m 42s) (53000 53%) 3.1043\n",
      "31m 49s (- 27m 6s) (54000 54%) 3.0555\n",
      "32m 25s (- 26m 31s) (55000 55%) 3.0869\n",
      "33m 0s (- 25m 56s) (56000 56%) 3.0365\n",
      "33m 36s (- 25m 20s) (57000 56%) 3.0008\n",
      "34m 11s (- 24m 45s) (58000 57%) 3.0050\n",
      "34m 46s (- 24m 10s) (59000 59%) 3.0034\n",
      "35m 22s (- 23m 34s) (60000 60%) 2.8728\n",
      "35m 57s (- 22m 59s) (61000 61%) 2.9358\n",
      "36m 32s (- 22m 23s) (62000 62%) 3.0001\n",
      "37m 8s (- 21m 48s) (63000 63%) 2.8861\n",
      "37m 43s (- 21m 13s) (64000 64%) 2.9127\n",
      "38m 18s (- 20m 37s) (65000 65%) 2.9183\n",
      "38m 54s (- 20m 2s) (66000 66%) 2.8060\n",
      "39m 29s (- 19m 27s) (67000 67%) 2.8685\n",
      "40m 4s (- 18m 51s) (68000 68%) 2.8780\n",
      "40m 40s (- 18m 16s) (69000 69%) 2.8246\n",
      "41m 15s (- 17m 41s) (70000 70%) 2.7935\n",
      "41m 50s (- 17m 5s) (71000 71%) 2.7019\n",
      "42m 26s (- 16m 30s) (72000 72%) 2.8211\n",
      "43m 1s (- 15m 54s) (73000 73%) 2.7089\n",
      "43m 36s (- 15m 19s) (74000 74%) 2.7329\n",
      "44m 12s (- 14m 44s) (75000 75%) 2.6978\n",
      "44m 47s (- 14m 8s) (76000 76%) 2.7594\n",
      "45m 22s (- 13m 33s) (77000 77%) 2.7556\n",
      "45m 57s (- 12m 57s) (78000 78%) 2.6294\n",
      "46m 33s (- 12m 22s) (79000 79%) 2.6813\n",
      "47m 8s (- 11m 47s) (80000 80%) 2.6357\n",
      "47m 43s (- 11m 11s) (81000 81%) 2.6823\n",
      "48m 19s (- 10m 36s) (82000 82%) 2.5929\n",
      "48m 54s (- 10m 1s) (83000 83%) 2.5748\n",
      "49m 30s (- 9m 25s) (84000 84%) 2.5656\n",
      "50m 5s (- 8m 50s) (85000 85%) 2.5299\n",
      "50m 41s (- 8m 15s) (86000 86%) 2.5415\n",
      "51m 16s (- 7m 39s) (87000 87%) 2.5648\n",
      "51m 51s (- 7m 4s) (88000 88%) 2.4923\n",
      "52m 27s (- 6m 28s) (89000 89%) 2.5339\n",
      "53m 2s (- 5m 53s) (90000 90%) 2.5626\n",
      "53m 38s (- 5m 18s) (91000 91%) 2.4817\n",
      "54m 13s (- 4m 42s) (92000 92%) 2.5325\n",
      "54m 49s (- 4m 7s) (93000 93%) 2.4563\n",
      "55m 24s (- 3m 32s) (94000 94%) 2.4773\n",
      "56m 0s (- 2m 56s) (95000 95%) 2.5010\n",
      "56m 36s (- 2m 21s) (96000 96%) 2.3930\n",
      "57m 11s (- 1m 46s) (97000 97%) 2.4147\n",
      "57m 46s (- 1m 10s) (98000 98%) 2.4431\n",
      "58m 21s (- 0m 35s) (99000 99%) 2.3924\n",
      "58m 57s (- 0m 0s) (100000 100%) 2.4465\n",
      "> 你可以幫我寄這封信嗎 ?\n",
      "= will you mail this letter for me ?\n",
      "< could you mind this letter this letter ? <EOS>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/upload/rt/00FF7041ECF44C49ACCF2C8C41993884/sgayzj45t5.png\">"
      ],
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "teacher_forcing_ratio = 0.5\n",
    "clip = 5.0\n",
    "\n",
    "\n",
    "def train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion,\n",
    "          max_length=MAX_LENGTH):\n",
    "    # Zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0  # Added onto for each word\n",
    "\n",
    "    # Get size of input and target sentences\n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "\n",
    "    # Run words through encoder\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "\n",
    "    # Prepare input and output variables\n",
    "    decoder_input = torch.LongTensor([[SOS_token]])\n",
    "    decoder_hidden = encoder_hidden  # Use last hidden state from encoder to start decoder\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "\n",
    "    # Choose whether to use teacher forcing\n",
    "    use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "    if use_teacher_forcing:\n",
    "\n",
    "        # Teacher forcing: Use the ground-truth target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            decoder_input = target_variable[di]  # Next target is next input\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use network's own prediction as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "\n",
    "            # Get most likely word index (highest value) from output\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            ni = topi[0][0]\n",
    "\n",
    "            decoder_input = torch.LongTensor([[ni]])  # Chosen word is next input\n",
    "            if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "\n",
    "            # Stop at end of sentence (not necessary when using known targets)\n",
    "            if ni == EOS_token: break\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))\n",
    "\n",
    "\n",
    "hidden_size = 500\n",
    "n_layers = 2\n",
    "dropout_p = 0.05\n",
    "\n",
    "\n",
    "# Initialize models\n",
    "encoder = BioEncoderLSTM(input_lang.n_words, hidden_size, n_layers)\n",
    "decoder = DecoderLSTM(hidden_size, output_lang.n_words, n_layers*2, dropout_p=dropout_p)\n",
    "\n",
    "\n",
    "if USE_CUDA:\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "learning_rate = 0.0001\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "\n",
    "# Configuring training\n",
    "n_epochs = 100000\n",
    "plot_every = 200\n",
    "print_every = 1000\n",
    "\n",
    "# Keep track of time elapsed and running averages\n",
    "start = time.time()\n",
    "plot_losses = []\n",
    "print_loss_total = 0 # Reset every print_every\n",
    "plot_loss_total = 0 # Reset every plot_every\n",
    "\n",
    "\n",
    "# Begin!\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "    # Get training data for this cycle\n",
    "    training_pair = variables_from_pair(random.choice(pairs))\n",
    "    input_variable = training_pair[0]\n",
    "    target_variable = training_pair[1]\n",
    "\n",
    "    # Run the train function\n",
    "    loss = train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "\n",
    "    # Keep track of loss\n",
    "    print_loss_total += loss\n",
    "    plot_loss_total += loss\n",
    "\n",
    "    if epoch == 0: continue\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print_summary = '%s (%d %d%%) %.4f' % (\n",
    "        time_since(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg)\n",
    "        print(print_summary)\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        plot_loss_avg = plot_loss_total / plot_every\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def show_plot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2) # put ticks at regular intervals\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "show_plot(plot_losses)\n",
    "\n",
    "def evaluate(sentence, max_length=MAX_LENGTH):\n",
    "    input_variable = variable_from_sentence(input_lang, sentence)\n",
    "    input_length = input_variable.size()[0]\n",
    "\n",
    "    # Run through encoder\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "\n",
    "    # Create starting vectors for decoder\n",
    "    decoder_input = torch.LongTensor([[SOS_token]])  # SOS\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "    # Run through decoder\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        # Choose top word from output\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == EOS_token:\n",
    "            decoded_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(output_lang.index2word[ni.item()])\n",
    "\n",
    "        # Next input is chosen word\n",
    "        decoder_input = torch.LongTensor([[ni]])\n",
    "        if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_randomly():\n",
    "    pair = random.choice(pairs)\n",
    "\n",
    "    output_words = evaluate(pair[0])\n",
    "    output_sentence = ' '.join(output_words)\n",
    "\n",
    "    print('>', pair[0])\n",
    "    print('=', pair[1])\n",
    "    print('<', output_sentence)\n",
    "    print('')\n",
    "\n",
    "evaluate_randomly()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "id": "585E40B2A35C4427AF86361189EDB98C",
    "jupyter": {},
    "notebookId": "66406df19ffe3bb7f164d64c",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def sample_test_dataset(size=100):\n",
    "    with open('cn-eng-test.txt', 'w+') as f:\n",
    "        f.write('\\n'.join(['\\t'.join(pair) for pair in random.sample(pairs, k=size)]))\n",
    "\n",
    "sample_test_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "id": "F15202ACAA414A2B9C611A61EDC2ACE7",
    "jupyter": {},
    "notebookId": "66406df19ffe3bb7f164d64c",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset bleu score: 0.15587388613826067\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "\n",
    "# 读取测试数据集\n",
    "with open('/home/mw/project/cn-eng-test.txt') as f:\n",
    "    lines = f.read().strip().split('\\n')\n",
    "    \n",
    "    test_pairs = [[normalize_string(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "\n",
    "\n",
    "test_pairs_dict = collections.defaultdict(lambda : [])\n",
    "\n",
    "for pair in test_pairs:\n",
    "    test_pairs_dict[pair[0]].append(pair[1].split(' '))\n",
    "\n",
    "\n",
    "def evaluate_bleu_score():\n",
    "    candicates = []\n",
    "    references = []\n",
    "\n",
    "    for i, pair in enumerate(test_pairs_dict.items(), start=1):\n",
    "        candicate = evaluate(pair[0])\n",
    "        if candicate[-1] == '<EOS>':\n",
    "            candicate.pop(-1)\n",
    "        candicates.append(candicate)\n",
    "        references.append(pair[1])\n",
    "    \n",
    "    score = bleu_score(candicates, references)\n",
    "    return score\n",
    "\n",
    "print('test dataset bleu score: %s' % evaluate_bleu_score())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "id": "A73F65DE29384C8B8DCBFDDE8414D946",
    "jupyter": {},
    "notebookId": "66406df19ffe3bb7f164d64c",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def translate_test_file(test_file_path, output_file_path):\n",
    "    # 读取测试文件\n",
    "    with open(test_file_path, 'r', encoding='utf-8') as test_file:\n",
    "        lines = test_file.readlines()\n",
    "    \n",
    "    # 准备输出文件\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "        # 对每一行（句子）进行翻译\n",
    "        for line in lines:\n",
    "            line = line.strip()  # 去除可能的前后空格\n",
    "            if not line:  # 跳过空行\n",
    "                continue\n",
    "            # 使用模型进行翻译\n",
    "            output_words = evaluate(normalize_string(line))\n",
    "            output_sentence = ' '.join(output_words[:-1])  # 去除<EOS>标记\n",
    "            # 写入原句和翻译结果\n",
    "            output_file.write(f'{line}\\n{output_sentence}\\n\\n')\n",
    "            \n",
    "# 调用函数，传入测试文件路径和输出文件路径\n",
    "translate_test_file('/home/mw/input/cna8958/test.txt', '/home/mw/project/translated_test_LSTM.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
