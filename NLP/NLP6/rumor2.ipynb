{"cells":[{"cell_type":"markdown","metadata":{"trusted":true,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"D89B0EAB542642F98900CCC4D4E49360","runtime":{"status":"default","execution_status":null,"is_visible":false},"scrolled":false,"notebookId":"66912b24bf1332bcf37ab14f"},"source":"## 欢迎进入 ModelWhale Notebook  \n\n这里你可以编写代码，文档  \n\n### 关于文件目录  \n\n\n**project**：project 目录是本项目的工作空间，可以把将项目运行有关的所有文件放在这里，目录中文件的增、删、改操作都会被保留  \n\n\n**input**：input 目录是数据集的挂载位置，所有挂载进项目的数据集都在这里，未挂载数据集时 input 目录被隐藏  \n\n\n**temp**：temp 目录是临时磁盘空间，训练或分析过程中产生的不必要文件可以存放在这里，目录中的文件不会保存  \n"},{"cell_type":"code","metadata":{"id":"810CF482643D4985AFA558B9F60AF78D","notebookId":"66912b24bf1332bcf37ab14f","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"source":"# 试试这个经典示例\nprint (\"hello ModelWhale\")","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"trusted":true,"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"4FC61B519BE24DD590C9913A15FBDA95","scrolled":false,"notebookId":"66912b24bf1332bcf37ab14f"},"source":"# 查看个人持久化工作区文件\n!ls /home/mw/project/","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"trusted":true,"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"16BEDEF83DF24B65ABB0E52615CECDCB","scrolled":false,"notebookId":"66912b24bf1332bcf37ab14f"},"source":"# 查看当前挂载的数据集目录\n!ls /home/mw/input/","outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"BD66C191DA4B409A8B4864A11F35E21C","notebookId":"66912b24bf1332bcf37ab14f","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"# 数据预处理"},{"cell_type":"code","metadata":{"id":"17A8EB22F8F244FF89D30FCF5E6688CD","notebookId":"66912b24bf1332bcf37ab14f","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"import os\nimport json\nimport pandas as pd\n\n# 文件路径\nweibo_dir = '/home/mw/input/NLP61475/rumor/rumor/rumor_weibo'\nforward_comment_dir = '/home/mw/input/NLP61475/rumor/rumor/rumor_forward_comment'\n\n# 初始化数据列表\nweibo_data = []\nforward_comment_data = []\n\n# 处理rumor_weibo文件夹中的数据\nfor filename in os.listdir(weibo_dir):\n    if filename.endswith('.json'):\n        filepath = os.path.join(weibo_dir, filename)\n        with open(filepath, 'r', encoding='utf-8') as file:\n            data = json.load(file)\n            weibo_data.append(data)\n\n# 处理rumor_forward_comment文件夹中的数据\nfor filename in os.listdir(forward_comment_dir):\n    if filename.endswith('.json'):\n        filepath = os.path.join(forward_comment_dir, filename)\n        with open(filepath, 'r', encoding='utf-8') as file:\n            data = json.load(file)\n            # 提取rumorCode\n            rumor_code = filename.split('_')[1].split('.')[0]\n            for comment in data:\n                comment['rumorCode'] = rumor_code  # 添加rumorCode以便后续匹配\n                forward_comment_data.append(comment)\n\n# 转换为DataFrame\nweibo_df = pd.DataFrame(weibo_data)\nforward_comment_df = pd.DataFrame(forward_comment_data)\nweibo_df.to_csv('/home/mw/work/NLP6/weibo_data.csv', index=False, encoding='utf-8')\nforward_comment_df.to_csv('/home/mw/work/NLP6/forward_comment_data.csv', index=False, encoding='utf-8')\n","outputs":[],"execution_count":3},{"cell_type":"markdown","metadata":{"id":"3EC5D9CE893A46F884293B3DA3890EE5","notebookId":"66912b24bf1332bcf37ab14f","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":""},{"cell_type":"markdown","metadata":{"id":"1A0C21F4220C45E5AB34D8AD3A55003A","notebookId":"66912b24bf1332bcf37ab14f","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"# 这部分是实现了谣言检测系统 将预训练模型"},{"cell_type":"code","metadata":{"id":"B9BA446BBC1E4E5C962C25E4AF757D47","notebookId":"66912b24bf1332bcf37ab14f","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"\n\nimport json\nimport pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, models, util\nfrom transformers import pipeline, BertTokenizer, BertForTokenClassification\nimport torch\n\n# 加载微博谣言数据和辟谣数据集\ndef load_data():\n    # 加载谣言数据\n    rumor_df = pd.read_csv('/home/mw/work/NLP6/weibo_data.csv')\n    \n    # 加载辟谣数据\n    fact_data = []\n    with open('/home/mw/input/NLP61475/rumor/rumor/fact.json', 'r', encoding='utf-8') as f:\n        for line in f:\n            fact_data.append(json.loads(line.strip()))\n    \n    fact_df = pd.DataFrame(fact_data)\n    fact_df = fact_df.dropna(subset=['title'])\n    \n    return rumor_df, fact_df\n\ndef extract_entities(text):\n    \"\"\"提取命名实体\"\"\"\n    entities = ner_pipeline(text)\n    return {entity['word'] for entity in entities}\n\ndef entity_similarity(text1, text2):\n    \"\"\"计算命名实体相似度\"\"\"\n    entities1 = extract_entities(text1)\n    entities2 = extract_entities(text2)\n    if not entities1 or not entities2:\n        return 0.0\n    intersection = entities1.intersection(entities2)\n    union = entities1.union(entities2)\n    return len(intersection) / len(union)\n\ndef combined_similarity(text1, text2):\n    \"\"\"结合句子嵌入相似度和实体相似度\"\"\"\n    embed_sim = util.pytorch_cos_sim(model.encode([text1], convert_to_tensor=True), \n                                     model.encode([text2], convert_to_tensor=True)).item()\n    entity_sim = entity_similarity(text1, text2)\n    return 0.5 * embed_sim + 0.5 * entity_sim\n\ndef debunk_rumor(input_rumor):\n    \"\"\"谣言检测\"\"\"\n    similarity_scores = [combined_similarity(input_rumor, fact_text) for fact_text in fact_df['title']]\n    \n    most_similar_index = np.argmax(similarity_scores)\n    most_similar_fact = fact_df.iloc[most_similar_index]\n    \n    print(\"微博谣言:\", input_rumor)\n    print(f\"辟谣判断：{most_similar_fact['explain']}\")\n    print(f\"辟谣依据：{most_similar_fact['title']}\")\n\nif __name__ == \"__main__\":\n    # 加载数据\n    rumor_df, fact_df = load_data()\n    \n    # 定制路径加载SimCSE模型\n    simcse_path = '/home/mw/input/MINI_LLM3084'\n    word_embedding_model = models.Transformer(simcse_path)\n    pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n    model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n    model.to('cuda')\n    \n    # 定制路径加载预训练的NER模型\n    ner_model_path = '/home/mw/input/bert_base_case9058'\n    tokenizer = BertTokenizer.from_pretrained(ner_model_path)\n    ner_model = BertForTokenClassification.from_pretrained(ner_model_path)\n    ner_pipeline = pipeline('ner', model=ner_model, tokenizer=tokenizer, aggregation_strategy=\"simple\", device=0)\n    \n    # 举个例子进行谣言检测\n    weibo_rumor = \"据最新研究发现，此次新型肺炎病毒传播途径是华南海鲜市场进口的豺——一种犬科动物携带的病毒，然后传给附近的狗，狗传狗，狗传人。狗不生病，人生病。人生病后又传给狗，循环传染。\"\n    debunk_rumor(weibo_rumor)","outputs":[{"output_type":"stream","name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm, trange\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at /home/mw/input/bert_base_case9058 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"},{"output_type":"stream","name":"stdout","text":"微博谣言: 据最新研究发现，此次新型肺炎病毒传播途径是华南海鲜市场进口的豺——一种犬科动物携带的病毒，然后传给附近的狗，狗传狗，狗传人。狗不生病，人生病。人生病后又传给狗，循环传染。\n辟谣判断：尚无定论\n辟谣依据：狗能感染新型冠状病毒\n"}],"execution_count":1},{"cell_type":"code","metadata":{"id":"B217C32FD7FC4F4F9095A2DABB0733EF","notebookId":"66912b24bf1332bcf37ab14f","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"import pandas as pd\n\n# 读取CSV文件的前10000行数据并保存\ndef read_and_save_first_10000_rows(file_path, output_file):\n    df = pd.read_csv(file_path, nrows=100000)\n    df.to_csv(output_file, index=False)  # 将数据保存到新文件，不包含索引列\n\n# 示例使用\ninput_file = '/home/mw/work/NLP6/positive_comments.csv'  # 替换成你的CSV文件路径\noutput_file = '/home/mw/work/NLP6/p_comments_100000.csv'  # 替换成你想保存的CSV文件路径\n\nread_and_save_first_10000_rows(input_file, output_file)\n","outputs":[],"execution_count":11},{"cell_type":"code","metadata":{"id":"6C15AF2634E348549EAF51DED4B07956","notebookId":"66912b24bf1332bcf37ab14f","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"import pandas as pd\n\n# 读取CSV文件的前10000行数据并保存\ndef read_and_save_first_10000_rows(file_path, output_file):\n    df = pd.read_csv(file_path, nrows=100000)\n    df.to_csv(output_file, index=False)  # 将数据保存到新文件，不包含索引列\n\n# 示例使用\ninput_file = '/home/mw/work/NLP6/negative_comments.csv'  # 替换成你的CSV文件路径\noutput_file = '/home/mw/work/NLP6/n_comments_1000000.csv'  # 替换成你想保存的CSV文件路径\n\nread_and_save_first_10000_rows(input_file, output_file)\n","outputs":[],"execution_count":12}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","version":"3.5.2","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":0}