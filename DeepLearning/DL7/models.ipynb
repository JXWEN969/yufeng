{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一次测试成功模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import math\n",
    "\n",
    " \n",
    "class ResidualBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(in_features, 0.8),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(in_features, 0.8),\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        return x + self.conv_block(x)\n",
    " \n",
    " \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, scale_factor=2, num_residual_blocks=16):\n",
    "        super(Generator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=9, stride=1, padding=4,padding_mode='reflect', bias=True)\n",
    "        self.prelu = nn.PReLU()\n",
    "        self.residual_blocks = nn.Sequential(*[ResidualBlock(64) for _ in range(num_residual_blocks)])\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(64, 256, kernel_size=3, stride=1, padding=1,padding_mode='reflect', bias=True),\n",
    "            nn.PixelShuffle(scale_factor),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(64, 256, kernel_size=3, stride=1, padding=1,padding_mode='reflect', bias=True),\n",
    "            nn.PixelShuffle(scale_factor),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(64, 3, kernel_size=9, stride=1, padding=4, bias=True)\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.prelu(out)\n",
    "        residual = out\n",
    "        out = self.residual_blocks(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += residual\n",
    "        out = self.upsample(out)\n",
    "        return out\n",
    " \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.Dropout(),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.layer(x)\n",
    "        return out\n",
    "\n",
    "class TruncatedVGG19(nn.Module):\n",
    "    def __init__(self, i=5, j=4):\n",
    "        super(TruncatedVGG19, self).__init__()\n",
    "        vgg19 = models.vgg19(pretrained=True).features\n",
    "        self.features = nn.Sequential(*list(vgg19.children())[:((i-1)*5 + j)])\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.features(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型2.0版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features, kernel_size=3):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        # 定义一个卷积块，包括两个卷积层，两个批归一化层和一个PReLU激活函数\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=kernel_size, stride=1, padding=kernel_size//2),\n",
    "            nn.BatchNorm2d(in_features),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=kernel_size, stride=1, padding=kernel_size//2),\n",
    "            nn.BatchNorm2d(in_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 前向传播过程中，将输入x和卷积块的输出相加\n",
    "        return x + self.conv_block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, large_kernel_size=9, small_kernel_size=3, n_channels=64, n_blocks=16):\n",
    "        super(Generator, self).__init__()\n",
    "        # 定义生成器的各个组成部分，包括一个卷积层，一个PReLU激活函数，一个残差块，一个卷积层，一个批归一化层，以及一个上采样部分\n",
    "        self.conv1 = nn.Conv2d(3, n_channels, kernel_size=large_kernel_size, stride=1, padding=large_kernel_size//2, padding_mode='reflect', bias=True)\n",
    "        self.prelu = nn.PReLU()\n",
    "        self.residual_blocks = nn.Sequential(*[ResidualBlock(n_channels, small_kernel_size) for _ in range(n_blocks)])\n",
    "        self.conv2 = nn.Conv2d(n_channels, n_channels, kernel_size=small_kernel_size, stride=1, padding=small_kernel_size//2, bias=True)\n",
    "        self.bn2 = nn.BatchNorm2d(n_channels)\n",
    "        # 假设上采样因子为2，如果需要可以进行相应的修改\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(n_channels, n_channels * (2 ** 2), kernel_size=small_kernel_size, stride=1, padding=small_kernel_size//2, padding_mode='reflect', bias=True),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(n_channels, n_channels * (2 ** 2), kernel_size=small_kernel_size, stride=1, padding=small_kernel_size//2, padding_mode='reflect', bias=True),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(n_channels, 3, kernel_size=large_kernel_size, stride=1, padding=large_kernel_size//2, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 定义生成器的前向传播过程\n",
    "        out = self.conv1(x)\n",
    "        out = self.prelu(out)\n",
    "        residual = out\n",
    "        out = self.residual_blocks(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += residual  # 元素级别的相加\n",
    "        out = self.upsample(out)\n",
    "        return out\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, kernel_size=3, n_channels=64, n_blocks=8, fc_size=1024):\n",
    "        super(Discriminator, self).__init__()\n",
    "        layers = []\n",
    "        # 假设在每个块后输入和输出通道数都翻倍\n",
    "        for i in range(n_blocks):\n",
    "            input_channels = n_channels if i == 0 else n_channels * (2 ** i)\n",
    "            output_channels = n_channels * (2 ** (i + 1))\n",
    "            layers.append(nn.Conv2d(input_channels, output_channels, kernel_size=kernel_size, stride=1 + i % 2, padding=kernel_size//2, bias=True))\n",
    "            layers.append(nn.BatchNorm2d(output_channels))\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "            if i % 2 == 1:  # 每隔一个块进行下采样\n",
    "                layers.append(nn.Conv2d(output_channels, output_channels, kernel_size=kernel_size, stride=2, padding=kernel_size//2, bias=True))\n",
    "                layers.append(nn.BatchNorm2d(output_channels))\n",
    "                layers.append(nn.LeakyReLU(0.2))\n",
    "\n",
    "        self.layer = nn.Sequential(*layers)\n",
    "        self.final = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(n_channels * (2 ** n_blocks), fc_size),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(fc_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 定义判别器的前向传播过程\n",
    "        out = self.layer(x)\n",
    "        out = self.final(out)\n",
    "        return out\n",
    "\n",
    "# TruncatedVGG19\n",
    "class TruncatedVGG19(nn.Module):\n",
    "    def __init__(self, i=5, j=4):\n",
    "        super(TruncatedVGG19, self).__init__()\n",
    "        vgg19 = models.vgg19(pretrained=True).features\n",
    "        self.features = nn.Sequential(*list(vgg19.children())[:((i-1)*5 + j)])\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加入SRRESNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import math\n",
    "\n",
    "class ConvolutionalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ConvolutionalBlock类定义了一个卷积块，包括卷积层、批量归一化层（如果需要）和激活层。\n",
    "    卷积层的参数（如输入/输出通道数、卷积核大小和步长）由构造函数参数提供。\n",
    "    批量归一化层是可选的，由batch_norm参数决定是否添加。\n",
    "    激活层的类型由activation参数决定，可以是PReLU、LeakyReLU、Tanh或者没有激活层。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, batch_norm=False, activation=None):\n",
    "        super(ConvolutionalBlock, self).__init__()\n",
    "\n",
    "        if activation is not None:\n",
    "            activation = activation.lower()\n",
    "            assert activation in {'prelu', 'leakyrelu', 'tanh'}\n",
    "\n",
    "        layers = list()\n",
    "\n",
    "        layers.append(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                      padding=kernel_size // 2))\n",
    "\n",
    "        if batch_norm is True:\n",
    "            layers.append(nn.BatchNorm2d(num_features=out_channels))\n",
    "\n",
    "        if activation == 'prelu':\n",
    "            layers.append(nn.PReLU())\n",
    "        elif activation == 'leakyrelu':\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "        elif activation == 'tanh':\n",
    "            layers.append(nn.Tanh())\n",
    "\n",
    "        self.conv_block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        前向传播函数接受一个输入张量，并应用卷积块的层到输入张量上，然后返回输出张量。\n",
    "        \"\"\"\n",
    "        output = self.conv_block(input)\n",
    "\n",
    "        return output\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    一个残差块，包含两个卷积块并通过残差连接。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size=3, n_channels=64):\n",
    "        \"\"\"\n",
    "        :param kernel_size: 卷积核大小\n",
    "        :param n_channels: 输入和输出通道数（相同，因为输入必须加到输出上）\n",
    "        \"\"\"\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        # 第一个卷积块\n",
    "        self.conv_block1 = ConvolutionalBlock(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel_size,\n",
    "                                              batch_norm=True, activation='PReLu')\n",
    "\n",
    "        # 第二个卷积块\n",
    "        self.conv_block2 = ConvolutionalBlock(in_channels=n_channels, out_channels=n_channels, kernel_size=kernel_size,\n",
    "                                              batch_norm=True, activation=None)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        前向传播。\n",
    "\n",
    "        :param input: 输入图像，尺寸为 (N, n_channels, w, h) 的张量\n",
    "        :return: 输出图像，尺寸为 (N, n_channels, w, h) 的张量\n",
    "        \"\"\"\n",
    "        residual = input  # (N, n_channels, w, h)\n",
    "        output = self.conv_block1(input)  # (N, n_channels, w, h)\n",
    "        output = self.conv_block2(output)  # (N, n_channels, w, h)\n",
    "        output = output + residual  # (N, n_channels, w, h)\n",
    "\n",
    "        return output\n",
    "\n",
    "class SubPixelConvolutionalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    SubPixelConvolutionalBlock类定义了一个子像素卷积块，包括卷积层、像素重排层和PReLU激活层。\n",
    "    卷积层的参数（如卷积核大小和输入/输出通道数）由构造函数参数提供。\n",
    "    像素重排层用于上采样，上采样倍数由scaling_factor参数决定。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size=3, n_channels=64, scaling_factor=2):\n",
    "        super(SubPixelConvolutionalBlock, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=n_channels, out_channels=n_channels * (scaling_factor ** 2),\n",
    "                              kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        self.pixel_shuffle = nn.PixelShuffle(upscale_factor=scaling_factor)\n",
    "        self.prelu = nn.PReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        前向传播函数接受一个输入张量，并应用子像素卷积块的层到输入张量上，然后返回输出张量。\n",
    "        \"\"\"\n",
    "        output = self.conv(input)\n",
    "        output = self.pixel_shuffle(output)\n",
    "        output = self.prelu(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "class SRResNet(nn.Module):\n",
    "    def __init__(self, large_kernel_size=9, small_kernel_size=3, n_channels=64, n_blocks=16, scaling_factor=4):\n",
    "        \"\"\"\n",
    "        初始化 SRResNet 模型的参数。\n",
    "\n",
    "        :param large_kernel_size: 输入和输出卷积层的卷积核大小\n",
    "        :param small_kernel_size: 中间卷积层的卷积核大小\n",
    "        :param n_channels: 卷积层的通道数\n",
    "        :param n_blocks: 残差块的数量\n",
    "        :param scaling_factor: 图像上采样的因子，必须是2、4或8\n",
    "        \"\"\"\n",
    "        super(SRResNet, self).__init__()\n",
    "\n",
    "        # 第一个卷积块，使用大卷积核，不使用批量归一化，激活函数为PReLU\n",
    "        self.conv_block1 = ConvolutionalBlock(in_channels=3, out_channels=n_channels, kernel_size=large_kernel_size,\n",
    "                                              batch_norm=False, activation='PReLu')\n",
    "\n",
    "        # 一系列残差块，每个块中包含一个跨块的跳跃连接\n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(kernel_size=small_kernel_size, n_channels=n_channels) for _ in range(n_blocks)])\n",
    "\n",
    "        # 第二个卷积块，使用小卷积核，使用批量归一化，不使用激活函数\n",
    "        self.conv_block2 = ConvolutionalBlock(in_channels=n_channels, out_channels=n_channels,\n",
    "                                              kernel_size=small_kernel_size,\n",
    "                                              batch_norm=True, activation=None)\n",
    "\n",
    "        # 一系列子像素卷积块，每个块的上采样因子为2，总的上采样因子等于scaling_factor\n",
    "        n_subpixel_convolution_blocks = int(math.log2(scaling_factor))\n",
    "        self.subpixel_convolutional_blocks = nn.Sequential(\n",
    "            *[SubPixelConvolutionalBlock(kernel_size=small_kernel_size, n_channels=n_channels, scaling_factor=2) for _\n",
    "              in range(n_subpixel_convolution_blocks)])\n",
    "\n",
    "        # 最后一个卷积块，使用大卷积核，不使用批量归一化，激活函数为Tanh\n",
    "        self.conv_block3 = ConvolutionalBlock(in_channels=n_channels, out_channels=3, kernel_size=large_kernel_size,\n",
    "                                              batch_norm=False, activation='Tanh')\n",
    "\n",
    "    def forward(self, lr_imgs):\n",
    "        \"\"\"\n",
    "        定义 SRResNet 的前向传播过程。\n",
    "\n",
    "        :param lr_imgs: 低分辨率输入图像，尺寸为 (N, 3, w, h) 的张量\n",
    "        :return: 超分辨率输出图像，尺寸为 (N, 3, w * 缩放因子, h * 缩放因子) 的张量\n",
    "        \"\"\"\n",
    "        output = self.conv_block1(lr_imgs)  # (N, 3, w, h)\n",
    "        residual = output  # (N, n_channels, w, h)\n",
    "        output = self.residual_blocks(output)  # (N, n_channels, w, h)\n",
    "        output = self.conv_block2(output)  # (N, n_channels, w, h)\n",
    "        output = output + residual  # (N, n_channels, w, h)\n",
    "        output = self.subpixel_convolutional_blocks(output)  # (N, n_channels, w * 缩放因子, h * 缩放因子)\n",
    "        sr_imgs = self.conv_block3(output)  # (N, 3, w * 缩放因子, h * 缩放因子)\n",
    "\n",
    "        return sr_imgs\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
