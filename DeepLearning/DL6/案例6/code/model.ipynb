{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderWithRNN(nn.Module):\n",
    "    # ...\n",
    "    def __init__(self, cfg, encoder_dim=14*14*2048):\n",
    "        super(DecoderWithRNN, self).__init__()\n",
    "        # 初始化层的其余部分...\n",
    "        self.embedding = nn.Embedding(cfg['vocab_size'], cfg['embed_dim'])\n",
    "        self.decode_step = nn.LSTMCell(cfg['embed_dim'], cfg['decoder_dim'])\n",
    "        self.init_h = nn.Linear(encoder_dim, cfg['decoder_dim'])  # 初始隐藏状态h0\n",
    "        self.init_c = nn.Linear(encoder_dim, cfg['decoder_dim'])  # 初始细胞状态c0\n",
    "        self.fc = nn.Linear(cfg['decoder_dim'], cfg['vocab_size'])\n",
    "        # 可能还需要其他层...\n",
    "\n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
    "        # 前向传播的其余部分...\n",
    "        h, c = self.init_h(encoder_out), self.init_c(encoder_out)  # (batch_size, decoder_dim)\n",
    "        for t in range(max(decode_lengths)):\n",
    "            batch_size_t = sum([l > t for l in decode_lengths])\n",
    "            embeddings = self.embedding(encoded_captions[:batch_size_t, t])\n",
    "            h, c = self.decode_step(embeddings, (h[:batch_size_t], c[:batch_size_t]))\n",
    "            preds = self.fc(h)  # (batch_size_t, vocab_size)\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "        return predictions, encoded_captions, decode_lengths, sort_ind\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    # ...\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # 编码器的线性层\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # 解码器的线性层\n",
    "        self.full_att = nn.Linear(attention_dim, 1)  # 用于计算注意力权重的线性层\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)  # softmax层来计算权重\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n",
    "        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n",
    "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n",
    "        alpha = self.softmax(att)  # (batch_size, num_pixels)\n",
    "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n",
    "        return attention_weighted_encoding, alpha\n",
    "\n",
    "class DecoderWithAttention(nn.Module):\n",
    "    # ...\n",
    "    def __init__(self, cfg, encoder_dim=2048):\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "        # 初始化层的其余部分...\n",
    "        self.attention = Attention(encoder_dim, cfg['decoder_dim'], cfg['attention_dim'])\n",
    "        self.embedding = nn.Embedding(cfg['vocab_size'], cfg['embed_dim'])\n",
    "        self.decode_step = nn.LSTMCell(cfg['embed_dim'] + encoder_dim, cfg['decoder_dim'])\n",
    "        self.init_h = nn.Linear(encoder_dim, cfg['decoder_dim'])  # 初始隐藏状态h0\n",
    "        self.init_c = nn.Linear(encoder_dim, cfg['decoder_dim'])  # 初始细胞状态c0\n",
    "        self.f_beta = nn.Linear(cfg['decoder_dim'], encoder_dim)  # 线性层以创建sigmoid激活门\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc = nn.Linear(cfg['decoder_dim'], cfg['vocab_size'])\n",
    "        # 可能还需要其他层...\n",
    "\n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
    "        # 前向传播的其余部分...\n",
    "        h, c = self.init_h(mean_encoder_out), self.init_c(mean_encoder_out)  # (batch_size, decoder_dim)\n",
    "        for t in range(max(decode_lengths)):\n",
    "            batch_size_t = sum([l > t for l in decode_lengths])\n",
    "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n",
    "                                                                h[:batch_size_t])\n",
    "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "            embeddings = self.embedding(encoded_captions[:batch_size_t, t])\n",
    "            h, c = self.decode_step(torch.cat([embeddings, attention_weighted_encoding], dim=1),\n",
    "                                    (h[:batch_size_t], c[:batch_size_t]))\n",
    "            preds = self.fc(h)  # (batch_size_t, vocab_size)\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "            alphas[:batch_size_t, t, :] = alpha\n",
    "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step(self, prev_embeddings, encoder_out, h, c):\n",
    "    \"\"\"Perform a single decoding step.\n",
    "\n",
    "    :param prev_embeddings: embeddings of previous words, shape: (batch_size, embed_dim)\n",
    "    :param encoder_out: encoded images, shape: (batch_size, num_pixels, encoder_dim)\n",
    "    :param h: previous hidden state, shape: (batch_size, decoder_dim)\n",
    "    :param c: previous cell state, shape: (batch_size, decoder_dim)\n",
    "    :return: preds - prediction scores for next word, shape: (batch_size, vocab_size)\n",
    "    :return: alpha - attention weights, shape: (batch_size, num_pixels)\n",
    "    :return: h - new hidden state, shape: (batch_size, decoder_dim)\n",
    "    :return: c - new cell state, shape: (batch_size, decoder_dim)\n",
    "    \"\"\"\n",
    "    attention_weighted_encoding, alpha = self.attention(encoder_out, h)\n",
    "    gate = self.sigmoid(self.f_beta(h))\n",
    "    attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "    h, c = self.decode_step(\n",
    "        torch.cat([prev_embeddings, attention_weighted_encoding], dim=1), (h, c)\n",
    "    )\n",
    "    preds = self.fc(h)\n",
    "    return preds, alpha, h, c"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
